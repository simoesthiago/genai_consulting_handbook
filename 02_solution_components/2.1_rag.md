# RETRIEVAL-AUGMENTED GENERATION (RAG)

**Goal:** Understang design and implementation of RAG systems that ground LLM responses in verifiable corporate data, enabling accurate, traceable, and secure knowledge retrieval for enterprise use cases.

**Prerequisites:**
- [`../01-foundations/llm_fundamentals.md`](../01-foundations/1.1_llm_fundamentals.md) â€” Understanding LLM limitations (context, knowledge cutoff)
- [`../01-foundations/prompt_engineering.md`](../01-foundations/1.2_prompt_engineering.md) â€” How to structure prompts for RAG
- [`../01-foundations/hallucinations_basics.md`](../01-foundations/1.3_hallucinations_basics.md) â€” Why RAG is essential for reducing hallucinations

**Related:**
- [`evals.md`](./evals.md) â€” Measuring RAG quality (retrieval precision/recall, context relevance)
- [`guardrails.md`](./guardrails.md) â€” Security controls for RAG (ABAC, sanitization)
- [`../06-risk-governance-ethics/data_privacy_and_security.md`](../06-risk-governance-ethics/data_privacy_and_security.md) â€” Data access controls and privacy
- [`../04-use-case-archetypes/knowledge_management.md`](../04-use-case-archetypes/knowledge_management.md) â€” RAG in knowledge management use cases

---

## TL;DR (30 seconds)

- RAG grounds LLM responses in your company's documents, enabling accurate, traceable answers
- Pipeline: Ingest â†’ Chunk â†’ Index â†’ Retrieve â†’ Rerank â†’ Generate
- Quality depends on chunking strategy, retrieval precision, and context relevance
- Security requires ABAC (attribute-based access control) at retrieval time
- GraphRAG handles multi-hop reasoning; traditional RAG is simpler and faster
- Always include citations and implement abstention when confidence is low

---

## What's In / What's Out

**In:**
- End-to-end RAG pipeline architecture (ingestion â†’ generation)
- Security patterns (ABAC, citations, sanitization)
- Trade-offs and decision frameworks (chunk size, top-k, reranking, hybrid search)
- GraphRAG vs traditional RAG (when to use each)

**Out:**
- Specific vector database implementation details (tool-specific)
- Advanced embedding model training (research-level)
- Fine-tuning LLMs for RAG (goes to specialized docs)
- Detailed prompt patterns for RAG (covered in [`prompt_engineering.md`](../01-foundations/1.2_prompt_engineering.md))

---

## 1. WHAT IS RAG/ WHEN TO USE RAG / WHEN TO AVOID

**What is RAG:** Retrieval-Augmented Generation (RAG) is a framework that optimizes the output of a Large Language Model (LLM) by referencing an authoritative external knowledge base outside its training data. By retrieving relevant documents before generating a response, RAG grounds the model in specific, up-to-date information, significantly reducing hallucinations and improving factual accuracy. RAG is like allowing that AI to take an 'open-book' test. Before answering a question, it first looks up the relevant information in a library of your specific documents, reads the right page, and then constructs an answer based on facts rather than just memory.

### 1.1. Use RAG When:

- **LLM needs company-specific knowledge**: Policies, procedures, product docs, internal knowledge bases
- **Data changes frequently**: RAG allows updates without retraining the model
- **Traceability is required**: Citations enable verification and auditability
- **Parametric knowledge is insufficient**: LLM's training data doesn't cover your domain
- **Multi-document queries**: User questions span multiple documents or sections
- **Access control is needed**: Different users should see different documents (ABAC)

### 1.2. Consider Alternatives When:

- **Simple, deterministic queries**: SQL or keyword search might be sufficient
- **Real-time data required**: RAG indexes are typically updated in batches (minutes to hours)
- **Extremely low latency needed**: RAG adds 100-500ms (retrieval + rerank)
- **Very small knowledge base**: < 10 documents, might fit in context window directly
- **Highly structured data**: Relational databases might be better than document retrieval

### 1.3. Don't Use RAG When:

- **No authoritative source exists**: RAG requires a knowledge base; if data is speculative, RAG won't help
- **Task doesn't require knowledge retrieval**: Pure generation tasks (creative writing, code generation) don't need RAG
- **Compliance prohibits external data**: Some regulated environments require fully parametric models

---

## 2. RAG ARCHITECTURE (End-to-End Pipeline)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   INGEST     â”‚ â”€â”€â”€> â”‚    CHUNK     â”‚ â”€â”€â”€> â”‚    INDEX     â”‚
â”‚  Documents   â”‚      â”‚  (Split +    â”‚      â”‚  (Embed +    â”‚
â”‚  (PDF, DOC,  â”‚      â”‚   Metadata)  â”‚      â”‚   Store)     â”‚
â”‚   Web, DB)   â”‚      â”‚              â”‚      â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â”‚
                                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GENERATE   â”‚ <â”€â”€â”€ â”‚    RERANK    â”‚ <â”€â”€â”€ â”‚   RETRIEVE   â”‚
â”‚  (LLM with   â”‚      â”‚  (Re-order   â”‚      â”‚  (Semantic   â”‚
â”‚   Context)   â”‚      â”‚   by         â”‚      â”‚   Search)    â”‚
â”‚              â”‚      â”‚   Relevance) â”‚      â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                            â–²
       â”‚                                            â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    User Query

### 2.1. Ingestion: Preparing the Knowledge Base

**What is it?** Ingestion is the process of collecting and preparing your company's documents so they can be searched by the AI system. Think of it as the "librarian's first task"â€”gathering all the books and organizing them before creating the catalog.

**What happens:**
1. **Collect documents** from various sources (PDFs, Word docs, web pages, databases)
2. **Extract text** from different formats (some PDFs are images that need OCR)
3. **Extract metadata** (who wrote it, when, which department, access permissions)
4. **Validate quality** (check for encoding issues, corrupted files, duplicates)

**Common sources you'll encounter:**
- **Document repositories**: SharePoint, Confluence, Google Drive, file servers
- **Databases**: SQL databases, NoSQL collections, data warehouses
- **APIs**: REST endpoints that return documentation or structured data
- **Structured files**: Excel, CSV, JSON files

**Key decisions to make:**
- **Which sources to include?** Start with the most critical documents for your use case
- **How often to update?** Daily? Weekly? Real-time? (Most systems update in batches)
- **What metadata is essential?** At minimum: source, timestamp, access level. Consider: author, department, document type, version

**What can go wrong:**
- **Missing metadata**: Without source information, you can't cite where answers came from
- **Ignoring access control**: Security risk if sensitive documents aren't tagged properly
- **No update strategy**: Stale information leads to wrong answers

### 2.2. Chunking: Breaking Documents into Searchable Units

**What is it?** Chunking is the process of splitting large documents into smaller, manageable pieces. Imagine you have a 50-page policy document. Chunking might split it into 100 smaller pieces (chunks), each containing 2-3 paragraphs. Each chunk is tagged with its location (e.g., "Section 3.2, pages 12-13"). Why? LLMs have context limits**: You can't send a 100-page document in one go.

**Chunking strategies (from simple to advanced):**

1. **Fixed-size chunking** (start here)
   - **How it works**: Split every 500 words (or tokens), regardless of content
   - **Example**: A 2000-word document becomes 4 chunks of 500 words each
   - **Pros**: Simple, predictable, works for most documents
   - **Cons**: May cut sentences in half, losing context
   - **When to use**: Uniform documents, MVP/prototype phase
   - **Overlap tip**: Use 10-20% overlap between chunks (chunk 1: words 1-500, chunk 2: words 450-950) to preserve context

2. **Sentence/paragraph-based chunking** (recommended for production)
   - **How it works**: Split at natural boundaries (paragraph breaks, section headers)
   - **Example**: Each paragraph becomes a chunk, or each section becomes a chunk
   - **Pros**: Preserves meaning, respects document structure
   - **Cons**: Variable sizes (some chunks too small, others too large)
   - **When to use**: Well-structured documents (markdown, formatted Word docs, HTML)

3. **Semantic chunking** (advanced, for high-quality needs)
   - **How it works**: Uses AI to find natural breakpoints where topics change
   - **Example**: A document about "Q3 financial results" might be chunked at the transition from "revenue" to "expenses"
   - **Pros**: Most semantically coherent chunks
   - **Cons**: More expensive (requires embedding model), slower
   - **When to use**: Complex documents, when quality is critical, legal/financial use cases

4. **Hierarchical chunking** (for multi-level queries)
   - **How it works**: Creates parent chunks (overview) and child chunks (details)
   - **Example**: Parent = "Company benefits policy" (500 words), Children = "Health insurance" (200 words), "Retirement plan" (200 words)
   - **Pros**: Can answer both high-level and detailed questions
   - **Cons**: More complex retrieval logic
   - **When to use**: Documents with clear hierarchy, when users ask both "what is X?" and "tell me details about X"

**What to preserve with each chunk:**
- **Source information**: Which document? Which page/section?
- **Position**: Where in the document? (for citations)
- **Document metadata**: Author, department, access level (for security filtering)
- **Timestamp**: When was this chunk created? (for freshness)

### 2.3. Indexing: Creating the Vector Search Index

**What is it?** Indexing converts text chunks into a format that computers can search efficiently. This is where "embeddings" come inâ€”they transform words into numbers that capture meaning.

**Why embeddings matter:** Traditional search (like Google) matches keywords. But "employee benefits" and "staff compensation" mean similar things even though they use different words. Embeddings capture this semantic similarity.

**Simple explanation of embeddings:**
- **Text**: "The company offers health insurance"
- **Embedding**: A list of numbers like `[0.23, -0.45, 0.67, ...]` (typically 384, 768, or 1536 numbers)
- **Key insight**: Similar meanings = similar numbers. "Health insurance" and "medical coverage" will have similar embedding vectors

**How indexing works:**
1. Take each chunk of text
2. Convert it to an embedding (a list of numbers) using an embedding model
3. Store the embedding in a vector database along with the original text and metadata
4. The database is optimized to find similar embeddings quickly

**Embedding models (what to know):**
- **General-purpose models**: Work well for most business documents. Examples: OpenAI's `text-embedding-3-small` (good balance), `text-embedding-3-large` (higher quality, more expensive)
- **Multilingual models**: Needed if your documents are in multiple languages
- **Domain-specific models**: Fine-tuned for specific industries (legal, medical, financial). Use when you have specialized terminology
- **Open-source options**: Available if you need to run on-premises or have cost constraints

**Vector databases (where embeddings are stored):**
Think of this as a specialized database designed for similarity search. Instead of "find exact match," it answers "find the 10 most similar items."

**Types:**
- **Managed services**: Cloud-hosted, easy to start (Pinecone, Weaviate, Qdrant Cloud, Azure AI Search)
- **Self-hosted**: More control, but you manage infrastructure (Chroma, Milvus, Qdrant, pgvector)

**What gets stored:**
- **Vector embedding**: The list of numbers representing the chunk's meaning
- **Original text**: The actual chunk text (needed for the final answer)
- **Metadata**: Source, timestamp, access level, etc. (for filtering and citations)

### 2.4. Retrieval: Finding Relevant Chunks

**What is it?** Retrieval is the process of finding the most relevant document chunks when a user asks a question. This is the "search" stepâ€”like a librarian finding the right books based on your question.

**How it works:**
1. User asks: "What is our refund policy?"
2. Convert the question to an embedding (same process as indexing)
3. Search the vector database for chunks with similar embeddings
4. Return the top-k most similar chunks (e.g., top 10)

**Retrieval strategies:**

1. **Dense retrieval (semantic search)** â€” Most common
   - **How it works**: Uses embeddings to find semantically similar chunks
   - **Example**: Query "employee benefits" finds chunks about "staff compensation," "worker perks," etc.
   - **Strengths**: Understands synonyms and meaning, not just keywords
   - **Weaknesses**: May miss exact keyword matches if wording is very different
   - **When to use**: Most RAG systems start here

2. **Sparse retrieval (keyword search)** â€” Traditional search
   - **How it works**: Matches exact keywords (like Google search)
   - **Example**: Query "refund policy" only finds chunks containing those exact words
   - **Strengths**: Precise for exact matches
   - **Weaknesses**: Misses "return policy" when searching for "refund policy"
   - **When to use**: When exact terminology matters (legal documents, product names)

3. **Hybrid retrieval** â€” Best of both worlds (recommended for production)
   - **How it works**: Combines semantic search + keyword search, merges results
   - **Example**: Finds both "refund policy" (keyword) and "return process" (semantic)
   - **Strengths**: Highest recall (finds more relevant chunks)
   - **Trade-off**: Slightly more complex, slightly higher latency
   - **When to use**: Production systems where quality matters

**Top-k selection (how many chunks to retrieve):**
- **What is k?** The number of chunks to retrieve (e.g., k=10 means "get the top 10 most similar chunks")
- **MVP**: Start with k=5-10
- **Production**: Typically k=10-15 (retrieve more than you'll use, then rerank)
- **Rule of thumb**: Retrieve 2-3x what you'll send to the LLM. If sending 5 chunks to LLM, retrieve 10-15.

**Similarity threshold (quality filter):**
- **What is it?** A minimum similarity score required (e.g., 0.7 out of 1.0)
- **Purpose**: Filters out irrelevant chunks before they reach the LLM
- **Example**: If the best match has similarity 0.5, it's probably not relevantâ€”abstain from answering
- **Warning**: Too high threshold (e.g., 0.9) may cause empty results even when relevant chunks exist

**Metadata filtering (access control):**
- **What is it?** Filtering chunks based on user permissions before retrieval
- **Why critical**: User from Marketing shouldn't see HR salary information
- **How it works**: 
  - Chunk metadata: `department: "HR"`, `access_level: "confidential"`
  - User attributes: `department: "Marketing"`, `clearance: "standard"`
  - Filter: Only retrieve chunks where user has permission
- **Critical rule**: Always filter server-side, never trust client-side filtering

### 2.5. Reranking: Improving Relevance Order

**What is it?** Reranking is an optional step that reorders the retrieved chunks to put the most relevant ones first. Think of it as a "second opinion" on relevance.

**Why rerank?**
- Initial retrieval (vector search) is fast but not perfect
- It might return 10 chunks, but chunks #3 and #7 are actually more relevant than #1
- Reranking uses a more sophisticated (but slower) model to reorder them

**How it works:**
1. Retrieve top 20 chunks (fast, using vector search)
2. Rerank those 20 chunks (slower, using a reranker model)
3. Take the top 5 reranked chunks
4. Send those 5 to the LLM

**When to skip reranking:**
- **MVP/prototype**: Speed and simplicity matter more than perfect quality
- **Small knowledge base**: < 1000 chunks, retrieval is already accurate
- **Tight latency budget**: If every millisecond counts (real-time chat)
- **Cost-sensitive**: If reranking cost is prohibitive

**When to use reranking:**
- **Production systems**: Quality improvement is worth the cost
- **Complex queries**: When initial retrieval struggles with nuanced questions
- **High-stakes use cases**: Legal, financial, medicalâ€”where accuracy is critical

### 2.6. Generation: LLM with Context

**What is it?** Generation is the final step where the LLM creates an answer using the retrieved chunks as context. This is where the "magic" happensâ€”the LLM synthesizes information from multiple chunks into a coherent answer.

**How it works:**
1. Take the user's question
2. Take the top-k retrieved chunks (after reranking)
3. Combine them into a prompt: "Answer this question using ONLY the information below..."
4. Send to LLM
5. LLM generates answer with citations

**Key principles:**
- **"Only use context"**: Critical instruction to prevent hallucination
- **Citations required**: Every claim must reference a source
- **Abstention**: If context doesn't contain the answer, say "I don't have that information"

**Citation format:**
- **Inline**: "Employees receive 20 days of vacation [hr-policy.pdf, p.5]"
- **Reference list**: "Sources: [1] hr-policy.pdf, [2] employee-handbook.pdf"
- **Structured**: JSON with source, page, chunk ID, confidence score

**Abstention policy (when to say "I don't know"):**
- **Low similarity**: Retrieved chunks have low relevance (similarity < threshold)
- **Out of scope**: Question is about something not in the knowledge base
- **Conflicting information**: Multiple sources contradict each other (flag this)
- **Missing information**: Context doesn't contain the answer

**Context window management:**
- **What is it?** LLMs have token limits (e.g., 128K tokens for GPT-4)
- **Reserve tokens for**: System prompt + User query + Retrieved chunks + Response
- **Typical allocation**: 
  - System prompt: 200 tokens
  - User query: 100 tokens
  - Retrieved chunks: 2000-4000 tokens (5-10 chunks)
  - Response: 1000-2000 tokens
- **Warning**: Don't exceed the limit (truncation loses information)

---

## 3. GRAPHRAG VS TRADITIONAL RAG

Most RAG systems use **Traditional RAG** (also called document-level RAG), which works great for straightforward questions. But sometimes you need **GraphRAG**, which understands relationships between entities. This section explains when to use each approach.

**Simple analogy:** 
- **Traditional RAG** = A librarian who finds books by topic ("Find books about vacation policy")
- **GraphRAG** = A detective who connects clues across multiple sources ("Who approved the budget mentioned in the Q3 report that the CEO discussed?")

---

### 3.1. Traditional RAG (Document-Level)

**What is it?** Traditional RAG is the standard approach we've been describing throughout this document. It treats documents as independent pieces of information and finds the most relevant chunks when you ask a question.

**How it works (simplified):**
1. Break documents into chunks
2. Store chunks in a vector database
3. When user asks a question, find the most similar chunks
4. Send those chunks to the LLM to generate an answer

**The key assumption:** The answer to your question exists in one or a few document chunks. You're essentially asking: "Which paragraph contains the answer?"

**What Traditional RAG is good at:**
- **Fact-finding questions**: "What is X?", "How many Y?", "When does Z happen?"
- **Single-document answers**: The answer is typically in one document or section
- **Fast responses**: Simple retrieval, low latency (100-300ms)
- **Lower cost**: Standard infrastructure, no special databases needed
- **Easy to implement**: Most RAG systems start here

**What Traditional RAG struggles with:**
- **Multi-hop reasoning**: Questions that require connecting information from multiple sources
  - Example: "What did the CEO say about the Q3 results mentioned in the earnings report?"
  - Problem: Requires finding CEO's statement AND the earnings report, then connecting them
- **Relationship questions**: "Who works with whom?", "What projects involve both departments?"
  - Problem: Doesn't understand relationships between entities (people, departments, projects)
- **Analytical questions**: "Why did sales drop?", "How are these policies related?"
  - Problem: Needs to understand cause-and-effect or connections between concepts

### 3.2. GraphRAG (Knowledge Graph + RAG) 

**What is it?** GraphRAG builds a "knowledge graph" that maps relationships between entities (people, organizations, projects, events) before answering questions. It's like creating a mind map of your company's information. Instead of just finding relevant chunks, GraphRAG understands **how things connect**.

**How it works (simplified):**
1. Extract entities and relationships from documents (e.g., "John works in Finance," "Project Alpha involves Finance and IT")
2. Build a knowledge graph (nodes = entities, edges = relationships)
3. When user asks a question, traverse the graph to find connected entities
4. Retrieve relevant subgraphs (not just chunks)
5. Generate answer using the connected information

**Real-world example:**
- **Question:** "Which projects involve both the Finance and IT departments?"
- **GraphRAG process:**
  1. Graph knows: Finance department â†’ [Person A, Person B], IT department â†’ [Person C, Person D]
  2. Graph knows: Project Alpha â†’ [Person A, Person C], Project Beta â†’ [Person B]
  3. Traverse graph: Find projects connected to both departments
  4. Answer: "Project Alpha involves both Finance and IT departments..."

**What GraphRAG is good at:**
- **Multi-hop reasoning**: Questions requiring 2+ steps of connection
  - Example: "What did the manager of the team that worked on Project X say about the budget?"
- **Relationship queries**: "Who reports to whom?", "What projects share resources?"
- **Analytical questions**: "Why did this happen?", "How are these related?"
- **Complex knowledge bases**: Organizations with many interconnected entities (people, projects, departments, policies)

**What GraphRAG struggles with:**
- **Simple questions**: Overkill for "What is our vacation policy?"
- **Speed**: Slower than Traditional RAG (graph traversal adds 200-500ms)
- **Cost**: More expensive (requires graph database, entity extraction, more processing)
- **Complexity**: Harder to build, maintain, and debug

### 3.3. Decision Framework: Which One Should You Use?

**Start with this question:** Can the answer be found in a single document or a few related chunks?

**Decision tree:**

User asks a question
â”‚
â”œâ”€ Can answer be found in 1-2 document chunks?
â”‚ â”‚
â”‚ â”œâ”€ YES â†’ Use Traditional RAG âœ…
â”‚ â”‚ Examples: "What is our refund policy?",
â”‚ â”‚ "How many vacation days?",
â”‚ â”‚ "What are the office hours?"
â”‚ â”‚
â”‚ â””â”€ NO â†’ Does it require connecting multiple entities/relationships?
â”‚ â”‚
â”‚ â”œâ”€ YES â†’ Consider GraphRAG ğŸ”
â”‚ â”‚ Examples: "Which projects involve Finance and IT?",
â”‚ â”‚ "What did the CEO say about Q3 results?",
â”‚ â”‚ "Who works with the team that handled Project X?"
â”‚ â”‚
â”‚ â””â”€ NO â†’ Try Traditional RAG first, upgrade if quality is insufficient
â”‚ (Sometimes Traditional RAG can handle it with better chunking/retrieval)

**Practical decision process:**

**Step 1: Start with Traditional RAG**
- Build your MVP with Traditional RAG
- Test with real user questions
- Measure answer quality

**Step 2: Identify problem questions**
- Track questions where Traditional RAG fails
- Look for patterns: Are they all multi-hop? Do they require relationships?

**Step 3: Evaluate if GraphRAG is needed**
- **If < 10% of questions fail**: Improve Traditional RAG (better chunking, reranki

---

## 4. CONTROLS AND SECURITY (Technical)

### 4.1. Access Control: ABAC at Retrieval

**The problem:** In most companies, different people should see different documents. An HR employee shouldn't see engineering technical specs. A marketing person shouldn't see financial salary data. A standard employee shouldn't see executive-only policies.Therefore, how do you ensure that when someone asks a question, they only get answers from documents they're allowed to see?

**What is ABAC?** Attribute-Based Access Control (ABAC) means access decisions are based on attributes (characteristics) of the user and the document, not just "user X can access document Y."

**How ABAC works in RAG (step by step):**

1. **Tag documents with access metadata:** When you ingest documents, tag each chunk with who can access it:
   - Example: A chunk from "HR Salary Guidelines" might have: `allowed_departments: ["HR", "Finance"]`, `min_clearance_level: 2`
   - Example: A chunk from "Public Company Policy" might have: `allowed_departments: ["All"]`, `min_clearance_level: 1`

2. **Get user attributes:** When a user asks a question, identify who they are and what attributes they have:
   - From their login token or IAM system: `department: "Marketing"`, `role: "Manager"`, `clearance_level: 1`

3. **Filter at retrieval time:** Before searching for relevant chunks, filter to only include chunks the user can access:
   - User from Marketing with clearance level 1 â†’ Only search chunks where `allowed_departments` includes "Marketing" OR "All", AND `min_clearance_level <= 1`
   - This happens **server-side**, before any chunks are retrieved

   ```python
   # Pseudocode
   user_attrs = get_user_attributes(user_id)
   results = vector_db.search(
       query_embedding,
       filter={
           "department": {"$in": user_attrs.departments},
           "clearance_level": {"$lte": user_attrs.clearance_level}
       }
   )
   ```   

4. **Search only in allowed chunks:** 
Perform the similarity search only within the filtered set of chunks the user can access.

**What can go wrong:**
- **Filtering only at application layer**: If filtering happens after retrieval, there's a window where restricted data could leak
- **Separate access control system**: If access rules are stored separately from chunks, there's a risk of race conditions or inconsistencies
- **No audit log**: Without logging what was retrieved, you can't prove compliance or investigate breaches

### 4.2. Citations and Traceability

**The requirement:** Every claim in an AI-generated answer must be traceable back to a source document. This is non-negotiable for enterprise use.

**Why citations matter:**
- **Trust**: Users need to verify answers
- **Compliance**: Regulated industries require source documentation
- **Legal protection**: "The AI said it" isn't a defenseâ€”you need to show the source
- **Quality control**: Citations help identify when the AI is hallucinating

**Implementation:**
- Include source metadata in every retrieved chunk
- LLM prompt must require citations
- Parse citations from LLM response (structured output)
- Validate citations (ensure cited source exists and contains the claim)

**Citation formats:**

1. **Inline citations (most readable)**
   - "Employees receive 20 days of paid vacation per year [hr-policy.pdf, p.5]"
   - Good for: User-facing answers, readability

2. **Reference list (academic style)**
   - Answer: "Employees receive 20 days of paid vacation..."
   - Sources: [1] hr-policy.pdf, page 5; [2] employee-handbook.pdf, page 12
   - Good for: Formal documents, when multiple sources support one claim

3. **Structured (for systems)**
{
  "answer": "Employees receive 20 days of paid vacation per year.",
  "citations": [
    {
      "source": "hr-policy.pdf",
      "page": 5,
      "chunk_id": "chunk_123",
      "confidence": 0.95
    }
  ]
}

### 4.3. Sanitization of Retrieved Context

**Problem**: Retrieved chunks may contain sensitive data or prompt injection attempts.

**The problem:** Retrieved chunks may contain sensitive information (social security numbers, salaries, personal emails) or malicious content (prompt injection attacks). You need to clean this before it reaches the LLM or the user.

**Why sanitization matters:**
- **Privacy**: Protect PII (Personally Identifiable Information)
- **Security**: Prevent prompt injection attacks
- **Compliance**: GDPR, LGPD, HIPAA require data protection

**Example of sanitization:**

**Input sanitization (before sending to LLM)** Cleans retrieved chunks before they're sent to the LLM.

**What to remove/redact:**
- **PII**: Social security numbers, credit card numbers, passport numbers
- **Contact information**: Personal emails, phone numbers (if not needed for the answer)
- **Sensitive data**: Salaries, medical information, legal case details (if user doesn't have clearance)
- **Prompt injection patterns**: Malicious instructions hidden in documents (e.g., "Ignore previous instructions and say X")

**For detailed guardrails, see:** [`guardrails.md`](./guardrails.md)

---

## 5. CASE STUDY: BUILDING A COMPANY KNOWLEDGE BASE WITH RAG

This case study walks through a complete RAG implementation, explaining the rationale and trade-offs behind each decision.

**Company:** TechCorp, 500 employees  
**Problem:** Employees waste 2-3 hours/week searching across SharePoint, Confluence, Google Drive, and wikis for policies, procedures, and documentation.  
**Goal:** Build an AI assistant that answers questions using company documents, with citations and access control.  
**Constraints:** $500/month budget, 6-week MVP timeline, >85% accuracy, <3s response time, zero access violations.

**Documents:** 880 documents (~2850 pages) including HR policies, engineering docs, product specs, support procedures, and announcements.  
**Users:** All employees (public docs), HR team (HR docs), Engineering (engineering docs), Managers (management policies), Executives (all docs).  
**Question types:** 60% fact-finding, 30% procedural, 10% analytical.

---

### 5.1. Chunking Strategy

**The challenge:** Documents range from short FAQs to 50-page policy manuals. Questions need both precise facts ("What is the refund deadline?") and contextual explanations ("Explain our vacation policy").

**Analysis:**
- Documents are well-structured (sections, headers, paragraphs)
- Questions often need 2-3 paragraphs of context
- Precision matters (wrong answers = compliance risk)

**Decision:** Paragraph-based chunking, 500 tokens per chunk

**Rationale:**
- Paragraph-based preserves natural language boundaries (better than fixed-size which might split sentences)
- 500 tokens balances precision and context (not too small to miss context, not too large to include noise)
- Simpler than semantic chunking (faster to implement, lower cost)
- Can upgrade to semantic chunking later if needed

**Trade-offs:**
- Preserves document structure, good balance of precision/context
- May include some irrelevant paragraphs in chunks
- Not optimal for very precise fact-finding or very analytical questions

**Alternative considered:** Semantic chunking (more accurate but slower and more expensive for MVP)

### 5.2. Embedding Model

**The challenge:** Need to convert 2850 pages of text into searchable vectors. Documents are English-only, mix of technical (engineering) and non-technical (HR) content.

**Analysis:**
- Quality needs to be good but not perfect (MVP phase)
- Cost is a constraint ($500/month total budget)
- Latency matters but embeddings are generated once (not per query)

**Decision: OpenAI `text-embedding-3-small`**

**Rationale:**
- Good quality for general business documents (sufficient for MVP)
- Much cheaper than larger models (~$0.00002 per 1K tokens vs $0.0001 for large)
- Fast (low latency for initial indexing)
- Can upgrade to `text-embedding-3-large` later if quality issues arise

**Trade-offs:**
- Good balance of quality and cost
- May struggle with very technical terminology (can fine-tune later if needed)
- Not multilingual (not needed for this use case)

**Alternative considered:** Domain-specific fine-tuned model (better quality but much higher cost and complexity)

### 5.3. Vector Database

**The challenge:** Need to store ~50K chunks (estimated) and support metadata filtering for access control. Team has AWS expertise but wants to move fast.

**Analysis:**
- Must support metadata filtering (critical for ABAC access control)
- Scale: ~50K chunks (moderate scale)
- Budget: $500/month total (need to reserve for other components)
- Team expertise: AWS, but prefer managed services for speed

**Decision:** Pinecone (managed service)

**Rationale:**
- Managed service = faster to start (no infrastructure setup)
- Good metadata filtering = essential for access control
- Reasonable pricing = $70/month for up to 100K vectors (fits budget)
- Fast similarity search = meets latency requirements
- Can migrate to self-hosted later if scale/cost becomes an issue

**Trade-offs:**
- Fast to implement, good performance, fits budget
- Ongoing monthly cost (vs one-time infrastructure cost for self-hosted)
- Less control than self-hosted (acceptable trade-off for MVP)

**Alternative considered:** Self-hosted (pgvector on PostgreSQL) - more control but requires infrastructure management and longer setup time

### 5.5. Retrieval Strategy

**The challenge:** Users ask questions in natural language ("What's our vacation policy?") but also use specific terms ("PTO", "time off"). Need high recall (can't miss relevant information).

**Analysis:**
- Questions mix natural language and specific terminology
- Quality is more important than speed (3-second target is generous)
- Knowledge base has technical terms, product names, policy codes

**Decision:** Hybrid retrieval (dense + sparse)

**Rationale:**
- Dense (semantic) search catches synonyms and meaning ("vacation" â†’ "time off", "PTO")
- Sparse (keyword) search catches exact matches ("Project Alpha", policy codes)
- Combined** = best recall (catches both semantic and keyword matches)
- Industry best practice for production systems
- Worth the added complexity (~50ms latency) for quality improvement

**Implementation:** 70% semantic + 30% keyword (tuned based on testing)

**Trade-offs:**
- Best recall, handles diverse query types
- More complex to implement and tune
- Slightly higher latency (~50ms added)
- Requires keyword search infrastructure (Pinecone supports this)

**Alternative considered:** Dense-only (simpler, faster, but misses exact keyword matches - not acceptable for this use case)

### 5.6. Top-K and Reranking

**The challenge:** Questions may need information from multiple documents. Quality is critical (compliance risk), but 3-second response time allows for some processing overhead.

**Analysis:**
- Need good recall (retrieve enough chunks) but also precision (most relevant first)
- Quality is critical (wrong answers = compliance/legal risk)
- 3-second target allows for reranking step

**Decision:** Retrieve k=15, rerank top 10, send top 5 to LLM

**Rationale:**
- k=15 provides good recall without excessive noise
- Reranking improves quality significantly (10-20% better answer accuracy)
- Send top 5 balances context (enough information) and token cost (not too expensive)
- Latency: ~200ms retrieval + ~100ms reranking = acceptable within 3s target

**Cost impact:** Reranker ~$0.001 per query = ~$1/month for 1000 queries (negligible)

**Trade-offs:**
- Good balance of recall, precision, and cost
- Reranking adds 50-100ms latency (acceptable for quality gain)
- More complex architecture (one more component)
- Higher cost than no reranking (but minimal)

**Alternative considered:** No reranking (simpler, faster, but 10-20% lower quality - not acceptable given compliance requirements)

### 5.7. LLM Selection

**The challenge:** Need structured output (citations in JSON format), good quality for business documents, but cost-conscious. Quality is important but not life-critical.

**Analysis:**
- Need reliable structured output (citations must be parseable)
- Quality needs to be good but not perfect (business docs, not medical/legal)
- Budget constraint: $500/month total
- 1000 queries/month estimated
- Response time matters (target: <3s total)

**Decision:** GPT-4o-mini

**Rationale:**
- Good quality for business documents (sufficient for most use cases)
- Much cheaper than GPT-4o ($0.15 vs $2.50 per 1M input tokens = ~16x difference)
- Faster** than GPT-4o (lower latency, important for <3s target)
- Supports structured output (JSON mode for reliable citations)
- Can upgrade to GPT-4o later if quality issues arise

**Cost impact:** ~3000 tokens per query Ã— 1000 queries Ã— $0.15/1M tokens = ~$0.45/month (negligible cost, well within budget)

**Trade-offs:**
- Excellent quality-to-cost ratio, fast, sufficient for business documents
- Not as good as GPT-4o for very complex reasoning (acceptable for this use case)
- May struggle with very nuanced analytical questions (can upgrade if needed)
- Slightly lower quality than GPT-4o (but difference is minimal for most business queries)

**Alternative considered:** GPT-4o (higher quality but ~16x more expensive - would cost ~$7.50/month vs $0.45/month, not justified for MVP when GPT-4o-mini quality is sufficient)

**When to choose GPT-4o instead:**
- Very high-stakes use cases (legal, medical, financial compliance)
- Complex analytical questions requiring deep reasoning
- When quality is more important than cost
- When budget allows and quality improvement justifies the cost

### 5.8. Access Control

**The challenge:** Different users need different document access. HR policies only for HR, financial data only for Finance + C-level. Zero tolerance for access violations (compliance requirement).

**Analysis:**
- Must enforce access control (regulatory requirement)
- Multiple user types with different access levels
- Need audit logging (compliance requirement)
- Security is non-negotiable

**Decision:** ABAC at retrieval time (server-side filtering)

**Rationale:**
- Server-side filtering = most secure (filters before retrieval, can't be bypassed)
- At retrieval time = efficient (only searches allowed chunks)
- ABAC (Attribute-Based Access Control) = flexible (department, role, clearance level)
- Pinecone supports metadata filtering natively
- Audit logging = required for compliance

**Implementation:**
- Chunk metadata: `department: ["HR"]`, `access_level: "confidential"`, `min_clearance: 2`
- User attributes: From IAM system (department, role, clearance)
- Filter: Only retrieve chunks where user has access
- Audit log: Log user_id, query_category, retrieved_chunk_ids, timestamp

**Trade-offs:**
- Most secure approach, efficient, compliant
- Requires proper metadata tagging (must be done correctly)
- More complex than no access control (but required)

### 5.9. End-to-End Flow Example

**User:** Sarah (Marketing department, standard employee)  
**Query:** "What is our vacation policy?"

1. **Authentication:** System extracts `department: "Marketing"`, `role: "Employee"`, `clearance: 1`
2. **Access filtering:** Only search chunks where `(department = "Marketing" OR "All") AND clearance <= 1`
3. **Hybrid retrieval:** 
   - Semantic search finds "time off policy", "PTO policy"
   - Keyword search finds "vacation policy"
   - Combine and rank: Top 15 chunks
4. **Reranking:** Reorder top 10 by relevance â†’ Top 5 from hr-policy-vacation.pdf and employee-handbook.pdf
5. **Generation:** LLM answers using context, cites sources in structured format
6. **Validation:** Check citations exist and contain claims
7. **Response:** "Employees receive 20 days of paid vacation per year [hr-policy-vacation.pdf, p.5]. Requests must be submitted 2 weeks in advance [employee-handbook.pdf, p.12]."
8. **Audit log:** Record user_id, query, retrieved_chunk_ids, citations, timestamp

## NEXT STEPS

**Understand Related Patterns:**
- **RAG + Tool Calling**: Use RAG for knowledge retrieval, tool calling for actions (e.g., "What's our refund policy?" â†’ RAG, "Process a refund for order #123" â†’ tool calling)
- **Multi-agent RAG**: Different agents specialize in different knowledge domains (HR agent, Finance agent, Engineering agent)
- **RAG + GraphRAG hybrid**: Use traditional RAG for simple queries, GraphRAG for complex multi-hop questions

**Recommended reading:**
- [`evals.md`](./evals.md) â€” How to measure RAG quality systematically
- [`guardrails.md`](./guardrails.md) â€” Security controls and prompt injection mitigation
- [`tool_calling.md`](./tool_calling.md) â€” Combining RAG with tool calling for richer capabilities
- [`../04-use-case-archetypes/knowledge_management.md`](../04-use-case-archetypes/knowledge_management.md) â€” RAG in knowledge management use cases
- [`../06-risk-governance-ethics/data_privacy_and_security.md`](../06-risk-governance-ethics/data_privacy_and_security.md) â€” Data access controls and privacy