# AGENTIC AI (ENTERPRISE AGENTS)

**Goal:** Help enterprise teams decide when agentic AI is justified, and how to design, deliver, and operate **agents that take actions** safely and reliably across tools, data, and workflows.

---

# TL;DR (30 SECONDS)

Agentic AI is not "a smarter chatbot". It is a **system that can plan and take actions** across tools to achieve an outcome, often over multiple steps and across multiple turns. This unlocks real enterprise value (end-to-end task completion), but it also expands risk: every tool becomes an action surface, every intermediate step can fail silently, and every ambiguity becomes an opportunity for errors or policy violations. The consulting-grade approach is to treat agentic AI as a product capability with explicit autonomy levels, a strict tool safety contract, measurable success criteria, and an operating loop where production failures become regression tests and release gates.

- **Agentic** means "plans + acts", not just "talks"
- Autonomy is a **spectrum**: most enterprise agents should start with **HITL** and earn autonomy with evidence
- Tooling is the risk surface: **allowlist + auth + validation + idempotency + truthful confirmation**
- The model proposes; the platform enforces: **server-side policy wins**
- Evals and observability are mandatory: multi-step systems fail silently without them
- If you cannot define P0 failures, you are not ready to ship an agent

---

# WHAT'S IN / WHAT'S OUT

**In:** definitions and decision criteria for agentic AI, core architecture patterns, the autonomy spectrum, a delivery playbook (POC -> pilot -> production), metrics and operating loops, pitfalls, and a detailed enterprise case study.

**Out:** vendor-specific frameworks, code-level implementations, and deep research discussions about planning algorithms. This page focuses on consulting-grade design and operating decisions that remain stable across stacks.

---

# 1. WHAT AGENTIC AI IS (AND WHAT IT IS NOT)

The term "agent" is overloaded. In consulting, you want a definition that maps to architecture and operating decisions. A practical definition is:

**An agentic system is a GenAI system that observes context, plans steps, executes actions through tools, and updates its behavior based on feedback, until an outcome is achieved or the system escalates.**

This definition is intentionally system-level. The "agent" is not the LLM alone. It is the orchestration around the LLM: tool gateways, state, policies, logging, and human approvals.

## 1.1. Agent vs Chatbot vs Workflow (A Useful Comparison)

Use this comparison to prevent category mistakes:

- A **chatbot** answers. It may retrieve documents, but it does not reliably complete tasks.
- A **workflow** executes. It follows predefined steps, but it struggles with ambiguous inputs and long tail cases.
- An **agent** bridges both: it can interpret ambiguous requests, choose tools, and navigate multi-step tasks, while the platform constrains what it is allowed to do.

This is why agentic AI is not a replacement for workflows. The enterprise design pattern is often: **workflow where you can, agent where you must.**

## 1.2. The Minimal Architecture of an Enterprise Agent

Even a "simple" agent has non-negotiable components:

- an **orchestrator** that controls the loop (what runs, when, and with which limits)
- a **tool registry and gateway** that enforces allowlists, authentication, and validation
- a **state model** for the task (what is known, what is missing, what is pending approval)
- **guardrails** that enforce policy boundaries at input, retrieval, tool, and output layers
- **observability** that makes every step traceable and diagnosable
- **evals** that measure multi-step success and block regressions

If you remove these, you do not have an enterprise agent. You have a demo.

## 1.3. When Not to Use Agentic AI

Agentic AI is usually the wrong tool when:

- the task can be expressed as a stable decision tree with structured inputs
- the cost of a mistake is high and the controls are not mature (no safe tool gateway, no approval model)
- the system cannot be audited (no traceability, unclear identity binding, no retention policy)
- the tool ecosystem is missing (no APIs, no permissions model, no idempotency)
- the expected volume is low and manual execution is cheaper than building an operating capability

The mature consulting move is to recommend a staged path: start with an assistant (L0/L1), then earn execution capability as the client builds the required controls.

---

# 2. WHY AGENTIC AI MATTERS IN ENTERPRISE (AND WHAT IT ENABLES)

In enterprise programs, the main limitation of "chatbots" is not intelligence; it is that they often stop at advice. Leaders do not only want recommendations, they want outcomes: "create the ticket", "draft the change request", "check the policy and route approvals", "update the CRM", "close the loop". Agentic AI is the path from "helpful text" to **end-to-end task completion**.

From a consulting perspective, agentic AI is valuable because it changes the operating model of knowledge work. It compresses workflows that used to require multiple systems and human coordination into a guided flow where the system does the busywork and humans approve what matters. The benefit is not only speed. It is also standardization: the same policy checks, the same forms, the same audit trail, every time.

The flip side is that agentic AI expands the blast radius of failure. A wrong answer is bad; a wrong action is worse. This is why agentic AI is fundamentally a **risk-managed engineering and operating discipline**, not a prompt-writing exercise.

## 2.1. Where Agents Create Value (And Where They Do Not)

Agentic approaches are most valuable when work is:

- multi-step (you must do A, then B, then C)
- cross-system (tickets, ERP, CRM, knowledge bases, identity systems)
- high variance (many edge cases, long tail requests, ambiguous inputs)
- constrained by policy (approvals, thresholds, roles, segregation of duties)
- measurable as an outcome (task completed, time saved, error reduced)

They are less valuable when the task is deterministic and can be solved with a strict workflow or RPA. If the logic is stable and the inputs are structured, a traditional workflow engine can be cheaper, faster, and safer. In consulting terms: **do not use an agent to replace a form if a form is the right product.**

## 2.2. Autonomy Is a Spectrum (And Most Enterprise Value Is Not "Full Autonomy")

"Agentic" does not mean "fully autonomous". In enterprise environments, the most common successful pattern is **progressive autonomy**: start with the agent proposing plans and drafts, then expand to executing low-risk actions, and only later enable higher-risk automation with approval gates.

A simple autonomy ladder that works in client conversations:

```text
L0: Answer-only assistant (no tools)
L1: Suggest actions (propose tool calls, human executes)
L2: Execute low-risk tools (read-only tools, reversible actions)
L3: Execute with approvals (risk-based HITL for high-impact steps)
L4: Autonomous execution (rare; requires strong controls and evidence)
```

The key consulting principle is that autonomy should be earned with evidence, not granted by enthusiasm. You expand autonomy when you can show stable eval results, strong guardrails, and reliable incident response.

## 2.3. The Decisions You Must Make Explicit (Or the Program Will Fail)

Most "agent failures" are actually decision failures: the program never agreed on boundaries, so the system behaves unpredictably and governance blocks it. Before building, align on:

- what outcomes matter (task success, cycle time, quality, adoption)
- what is disallowed (policy boundaries, data boundaries, tool boundaries)
- what is a **P0 failure** (release blocker) vs a tolerable P1/P2 in early phases
- who owns approvals and escalations (product, security, ops, SMEs)
- which autonomy level is acceptable now, and what evidence is needed to increase it

If these decisions are not written down, teams end up arguing about symptoms rather than improving the system.

---

# 3. HOW AGENTS WORK (SYSTEM VIEW)

Agents are easiest to build and operate when you treat them as a controlled loop around an LLM, not as an "autonomous mind". The loop is where you apply constraints: step budgets, tool allowlists, approvals, validation, and termination rules. Without those controls, agent behavior will be unpredictable, expensive, and hard to audit.

At a high level, an enterprise agent runs a cycle:

1. Observe: collect user intent, current task state, and relevant context.
2. Decide: choose the next best step (ask a question, retrieve knowledge, call a tool, escalate).
3. Act: execute the chosen step through a safe boundary.
4. Validate: check outcomes (tool success, policy compliance, schema correctness).
5. Repeat or stop: either continue or complete with a final response.

The most important consulting point is that the LLM should primarily be used for **interpretation and decision-making under ambiguity**, while the platform should handle **enforcement and execution semantics**.

## 3.1. The Agent Loop (Observe -> Plan -> Act -> Verify)

In practice, the loop must be explicit. Otherwise, teams cannot reason about cost, latency, or safety. A simple reference loop:

```text
User message
  -> Normalize + risk label
  -> Build working context (state + retrieval)
  -> LLM proposes next step(s)
  -> Platform validates proposal against policy
  -> Execute tool(s) through gateway (or ask user for approval)
  -> Record outcomes + update state
  -> Decide done vs continue
```

Where agents fail is usually in the "platform validates proposal" and "decide done vs continue" steps. If validation is weak, unsafe actions slip through. If termination is weak, agents thrash and burn tokens.

Two constraints that make agents manageable:

- **Budgets:** maximum steps, maximum tool calls, maximum tokens, maximum wall-clock time per request.
- **Termination rules:** explicit "done" conditions tied to the task state (not only to "the model feels done").

## 3.2. Planning and Control Patterns (Pick the Simplest That Works)

There is no single best agent architecture. The right pattern depends on risk appetite, latency constraints, and how predictable the workflow is. In enterprise delivery, the best default is to start with a constrained pattern and only add autonomy when the evidence supports it.

Common patterns:

- **ReAct-style (reason + act):** the agent alternates between thinking and tool calls. This is flexible, but can become chatty and expensive without budgets.
- **Plan-and-execute:** the agent produces a plan (steps), then executes step-by-step with validation. This improves traceability, but plans can be brittle if the world changes.
- **Graph/workflow-orchestrated agent:** a deterministic graph controls the main flow, and the model fills in ambiguous decisions (classification, extraction, routing). This is often the best enterprise pattern because it combines reliability with flexibility.
- **Supervisor/worker:** a supervisor decides which specialist to use (retrieval agent, tool agent, compliance agent). This is powerful but increases complexity and operational surface area (see `2.8_multi_agent_system.md`).

A practical decision rule:

- If the workflow is stable and high-stakes, prefer **deterministic orchestration** with LLM decisions inside narrow slots.
- If the workflow is ambiguous and low-to-medium risk, a more flexible loop can be justified, but enforce budgets and logging from day one.

## 3.3. Tool Boundaries: Where Safety and Reliability Are Won

Once an agent can call tools, it can change real systems. This is where most enterprise incidents happen. The safe pattern is to centralize tool execution behind a gateway that enforces:

- tool allowlists (small surface area)
- authentication and authorization (user identity + policy)
- argument validation (schema + business constraints)
- idempotency (avoid duplicates under retries/timeouts)
- truthful confirmation (no "done" unless the tool succeeded)

This is not optional. It is the contract described in `2.2_tool_calling.md`.

An important mental model: **the LLM proposes; the platform disposes**. The model may propose a tool call; the platform must decide whether it is allowed, rewrite it into a safe form, request approval, or block it.

## 3.4. State and Memory: Agents Need Structure, Not Just Chat History

Agents fail when they rely on raw chat history as "state". Chat history is noisy, long, and often includes unverified claims. Enterprise agents need a **task state model** that is explicit and structured: what is known, what is missing, what is approved, what is executed, what is pending.

A common pattern is to represent task state as a structured object (or database record) and treat the LLM as a planner that reads and updates that state:

- **Working state:** current task fields, validations, approvals, and tool outcomes.
- **Context memory:** relevant excerpts from conversation (summarized and bounded).
- **Knowledge retrieval:** grounded facts from enterprise sources (RAG) with provenance.

The key operational rule is to store only **validated facts**. If the model "guesses" a cost center or policy, that should not become memory. Memory must be earned the same way autonomy is earned: through validation and human approval where appropriate.

## 3.5. Error Handling and Escalation (Design for the 10% Edge Cases)

In enterprise reality, tools fail, data is missing, and users provide incomplete inputs. Agents must be designed to fail safely.

Safe escalation patterns:

- ask clarifying questions when critical fields are missing
- request approval for high-risk actions (risk-based HITL)
- fall back to read-only guidance when tools are unavailable
- escalate to a human queue when repeated tool failures occur

To avoid loops, enforce:

- max retries per tool call
- max total tool calls per request
- "stuck" detection (no state progress across N steps)

## 3.6. Threat Model for Agents (Injection Becomes an Action Risk)

Agents are more vulnerable than chatbots because prompt injection can lead to actions. Two practical enterprise assumptions reduce incidents:

1. User input and retrieved documents are **untrusted**.
2. Tool calls must be **validated and authorized server-side**, independent of what the model says.

This is why agentic AI must be built together with `2.4_guardrails.md`. Guardrails for an agent are not only about content moderation; they are about preventing unsafe actions, unauthorized data access, and operational leakage through logs.

---

# 4. HOW TO IMPLEMENT AGENTIC AI (DELIVERY PLAYBOOK)

Implementing an agent is not a single build task. It is a delivery program that must align product, engineering, security, and operations around an explicit autonomy target and an explicit risk model. The common failure mode is to prototype an agent that looks impressive, then discover that no one will approve it for production because it cannot be audited, controlled, or measured.

This playbook is written to avoid that outcome.

## 4.1. Step 0: Frame the Use Case as an Outcome With Constraints

Start with one sentence that expresses the outcome and the boundaries. Example:

"An employee can request a purchase order, and the system will collect missing information, validate policy and budget, route approvals, and create a requisition in the ERP, without exposing PII and without executing high-impact actions without approval."

That sentence forces the decisions that matter:

- What counts as success (task completed, not "assistant replied")?
- Which constraints are non-negotiable (privacy, approvals, time)?
- Which actions are allowed (and under what conditions)?

Then define:

- **Task inventory:** list the top 10-20 tasks the agent must complete, with frequency and business value.
- **Autonomy level (L0-L4):** what the agent is allowed to execute now.
- **P0/P1/P2 definitions:** what blocks release vs what is tolerable in pilot.
- **Slices:** which personas, regions, and workflows carry concentrated risk.

If you do not define these, the agent will be evaluated on vibes.

## 4.2. Step 1: Make Tools "Agent-Ready" (Most Programs Skip This and Pay Later)

Tools are where agentic programs succeed or fail. "We have APIs" is not enough. The platform needs tools that are safe to call under uncertainty.

Minimum tool readiness checklist:

- **Stable schemas:** explicit inputs/outputs with types and constraints (not free-form blobs).
- **Authorization model:** user identity binding and least privilege by default.
- **Idempotency:** safe retries without duplicate actions.
- **Observability:** structured logs for every call (request id, outcome, error codes).
- **Reversibility:** wherever possible, prefer reversible actions in early phases.

A consulting deliverable that accelerates alignment is a **tool registry** that lists, per tool:

- purpose and allowed intents
- required permissions and approval rules
- input schema and validation rules (including business constraints)
- side effects and rollback options
- idempotency strategy and failure semantics

This registry becomes the backbone for guardrails, evals, and audits.

## 4.3. Step 2: Design the Agent Architecture (Pick Constraints Before Intelligence)

Many teams start by choosing an LLM framework. In enterprise delivery, you should start by choosing constraints.

Design choices to make explicitly:

- **Orchestration pattern:** ReAct loop, plan-and-execute, or graph-controlled.
- **State model:** what is stored, what is derived, what is verified, what requires approval.
- **Tool gateway:** the single boundary where allowlists, auth, validation, and idempotency live.
- **Retrieval strategy (if needed):** which corpora, what freshness expectations, and how citations are enforced.
- **Termination rules:** step budgets and "done" criteria tied to state progress.

An agent that is slightly less "clever" but tightly bounded will outperform a clever agent that is unconstrained, because it will be cheaper, safer, and more diagnosable.

## 4.4. Step 3: Build Safety Into the Flow (Not as a Post-Filter)

For enterprise agents, safety is about actions and data, not only about content categories. Use the defense-in-depth logic from `2.4_guardrails.md` and the tool contract from `2.2_tool_calling.md`.

Non-negotiable safety controls:

- **Input and retrieval are untrusted:** treat injection as an action risk, not only as a content risk.
- **Server-side authorization:** tools run under enforced policy, not under model intent.
- **Approval gates:** risk-based HITL for irreversible or high-impact actions.
- **Output validation:** schemas, redaction, and truthful confirmations based on tool results.
- **Safe logging:** do not leak sensitive tool payloads and retrieved content into traces.

In practice, the safest early production posture is:

- read-only tools by default
- approvals for write tools above thresholds
- strict allowlists and constrained schemas
- explicit refusal UX for disallowed asks

This keeps adoption possible while controls mature.

## 4.5. Step 4: Build a v0 That Produces Evidence (Not Just Demos)

A v0 agent should be designed to generate learning quickly:

- start with a small task set (5-10 core workflows)
- use a constrained tool surface (read-only or reversible actions)
- capture failures as structured data (failure taxonomy)
- implement eval hooks from day one (see `2.5_evals.md`)

The goal is to learn where the agent breaks: missing data, ambiguous intents, brittle tools, incorrect retrieval, or unsafe behavior.

## 4.6. Step 5: Pilot With Progressive Autonomy

In pilot, you earn trust with stability and transparency:

- canary new prompt/config versions to a small cohort
- introduce HITL where risk is concentrated (high-cost actions, sensitive data)
- monitor false positives/negatives for guardrails and approvals
- iterate weekly: production signals -> labeled cases -> eval suite -> fixes

The key is to prevent "silent failure". If the agent fails, you need traces and metrics that explain why, not only a user complaint.

## 4.7. Step 6: Productionize the Operating Model

Production agentic systems require the same discipline as other production systems, plus additional controls:

- version prompts, tool schemas, and orchestration configs
- implement rollbacks and kill switches (disable tools, force HITL, limit intents)
- enforce budgets (tokens, steps, tool calls) to control cost and looping
- create runbooks for P0 incidents and tool outages

If changes are not versioned and gated, the agent will regress in ways the team cannot explain.

## 4.8. Consulting Deliverables (What Makes This Work Auditable)

A consulting-grade agentic AI engagement typically produces:

- a use case inventory with autonomy targets and ROI assumptions
- a risk model with P0/P1/P2 definitions and approval rules
- an agent architecture diagram and state model (what is verified vs assumed)
- a tool registry (schemas, permissions, idempotency, rollback)
- an eval plan (datasets, rubrics, gates) and an observability plan (metrics, traces, alerts)
- an operating cadence (weekly review, incident loop, change management)

These deliverables are how the program survives organizational churn and governance reviews.

---

# 5. HOW TO MEASURE AND OPERATE AGENTS (LLMOPS FOR ACTION SYSTEMS)

Agents are not real until they are measurable. In enterprise contexts, measurement is not only about product improvement; it is also about governance. Leaders need to know whether the agent is safe and worth scaling. Delivery teams need to know what is breaking and why. Both needs are met by the same operating capability: **evidence-based release gates** and **production telemetry that closes the loop**.

## 5.1. What to Measure (And Which Decisions Those Metrics Support)

The most useful metrics answer three questions:

1. Can we ship and expand autonomy safely?
2. Are users getting outcomes, not just responses?
3. Are we operating inside cost and latency budgets?

A practical metric set for enterprise agents:

- **Task success rate:** did the user reach the intended outcome (end-to-end), by slice.
- **Step success rate:** which step fails (retrieval, tool selection, tool execution, approval, formatting).
- **Tool health:** tool success rate, auth failures, validation failures, timeouts, retries, idempotency collisions.
- **Human intervention rate:** where and why humans intervened (approval thresholds, ambiguity, errors).
- **False confirmation rate:** how often the agent claimed success without tool success (should be near zero).
- **Safety incidents:** P0 count, categories, and examples (must be zero to ship).
- **Cost per successful outcome:** tokens + tool costs per completed task, not per request.
- **Latency by path:** with/without retrieval, with/without tools, with/without approvals.

The consulting discipline is to map each metric to a decision. Example:

- "We measure false confirmations because it determines whether write tools can be enabled without reputational risk."
- "We measure cost per successful outcome because it determines whether onboarding 10x more users is financially viable."

## 5.2. Evals for Agents: Multi-Step Is a Different Problem

Agent evaluation is harder than single-turn evaluation because failures can occur anywhere in the sequence. A good agent eval program separates:

- **Reasoning/planning errors:** wrong plan, wrong step order, missing required step.
- **Tool errors:** wrong tool selection, invalid arguments, failure handling, duplicate actions.
- **Knowledge errors:** wrong retrieval, missing citations, stale content decisions.
- **Policy errors:** unsafe actions, unauthorized access, leakage, bypass attempts.

Practical eval patterns that work:

- **Scenario-based evals:** end-to-end tasks with a rubric for each step and the final outcome.
- **Adversarial evals:** injection attempts, missing fields, ambiguous requests, "confused deputy" scenarios.
- **Regression suites:** every incident and every important bug becomes a test case.

For severity, reuse the language from `2.5_evals.md` and `2.4_guardrails.md`:

- **P0:** release blocker (unauthorized action, data leakage, access control violation, false confirmation).
- **P1:** serious failure tolerated only in early phases (wrong plan but blocked, missing citations where required).
- **P2:** minor issues (tone, minor formatting).

If you evaluate only the final response, you will miss the true failure modes of the agent.

## 5.3. Observability for Agents: Trace the Whole Loop

Agent observability must cover:

- the user request and slice metadata (persona, intent, risk label)
- the chosen plan and step budget
- each tool proposal and the platform decision (allowed, rewritten, blocked, approval requested)
- tool execution outcomes and error codes
- guardrail outcomes and redaction decisions
- termination conditions (why the agent stopped)

The purpose is not to store raw text everywhere. The purpose is to make the system diagnosable and auditable without leaking sensitive data. Use the privacy-by-design guidance from `2.6_observability_llomps.md`.

A common enterprise pattern is to log:

- structured signals by default (ids, lengths, classifications, outcomes)
- raw samples only under gated debug capture with short retention

Without a traceable loop, teams cannot distinguish "model issue" from "tool issue" from "policy issue", and improvement becomes slow.

## 5.4. Operating Cadence: The Weekly Loop That Makes Agents Improve

The simplest cadence that works in most enterprise programs:

- **Daily triage:** review P0/P1 events, tool outages, and repeated failure patterns.
- **Weekly improvement forum:** top failures by slice, false positives/negatives, cost trends, and backlog of fixes and eval additions.
- **Monthly governance review:** autonomy expansion decisions, audit posture, and readiness for new tools and use cases.

The key operating rule: every P0 incident becomes a regression test and a release gate. Otherwise, incidents repeat.

## 5.5. Change Management: Prompts, Tools, and Policies Are Production Changes

Agents are highly sensitive to small changes:

- prompt edits can change tool behavior
- retrieval config changes can change groundedness
- tool schema changes can break validation
- guardrail threshold changes can change refusals and adoption

Treat these as production changes:

- version them
- canary them
- gate them with evals
- monitor leading indicators during rollout
- roll back quickly when risk increases

This is the core of LLMOps for agents: safe change at speed.

---

# 6. PITFALLS AND FAILURE MODES (AND HOW TO DIAGNOSE THEM)

Agentic AI fails in predictable ways. The consulting goal is not to list generic risks, but to help teams diagnose what they see in production and connect symptoms to fixes and to prevention gates.

## 6.1. "The Agent Did Something We Did Not Intend" (Over-Autonomy)

**Symptom:** the agent executes an action that surprises stakeholders (wrong workflow, wrong scope, wrong user intent).

**Root cause:** autonomy boundaries were not explicit, and tool authorization relied on model intent rather than policy.

**Fix:** enforce server-side authorization by user identity and task state; add approval gates for high-impact actions; narrow the tool allowlist.

**Prevention:** define autonomy levels (L0-L4), make them visible in the UI, and gate autonomy expansion on eval evidence.

## 6.2. "It Said It Worked, But Nothing Happened" (False Confirmation)

**Symptom:** users lose trust because the agent claims success when the tool failed, was blocked, or timed out.

**Root cause:** tool execution results are not validated, and confirmations are generated without checking real outcomes.

**Fix:** enforce truthful confirmation: response content must depend on tool success signals; implement idempotency and clear failure handling.

**Prevention:** add a "false confirmation" metric and treat it as near-P0 for write tools. Add regression tests for tool timeouts and partial failures.

## 6.3. Infinite Loops and Thrashing (The Budget Problem)

**Symptom:** the agent keeps trying variations of the same step, burns tokens, and delays responses.

**Root cause:** missing termination rules, missing budgets, and no "stuck" detection.

**Fix:** set step/tool/token budgets, detect lack of state progress, and escalate to a human or a fallback mode when stuck.

**Prevention:** include loop scenarios in evals and create alerts for unusual step counts and token usage by slice.

## 6.4. "Memory Became the Problem" (Contamination and Privacy Risk)

**Symptom:** the agent reuses incorrect information, or sensitive information appears in later conversations.

**Root cause:** raw conversation history is treated as truth, and long-term memory stores unverified or sensitive data.

**Fix:** separate structured state (validated) from narrative context (summarized); store only validated facts; apply redaction and retention rules.

**Prevention:** treat memory as a governed data store: define what is allowed to be stored, for how long, and who can access it.

## 6.5. Prompt Injection Turns Into Action (Untrusted Inputs)

**Symptom:** the agent follows malicious instructions from user input or retrieved documents and attempts unsafe actions.

**Root cause:** the system treats retrieved text as trusted instructions, and tool calls are not validated against policy.

**Fix:** treat all external text as untrusted; isolate system instructions; validate tool calls server-side; apply guardrails at retrieval and tool boundaries.

**Prevention:** include injection cases in eval suites and monitor for "suspicious tool intent" patterns in production.

## 6.6. "We Can't Explain Why It Changed" (No Attribution)

**Symptom:** quality changes after an update, but teams cannot identify which change caused it.

**Root cause:** prompts, retrieval configs, tool schemas, and guardrails are not versioned and tagged in telemetry.

**Fix:** version everything and attach version tags to every request trace; use canaries and rollbacks.

**Prevention:** require version tags, eval gates, and rollback paths as part of release checklists.

## 6.7. Complexity Explosion (The Multi-Agent Temptation)

**Symptom:** the system becomes hard to operate: too many components, unclear ownership, and unpredictable interactions.

**Root cause:** the program introduced multiple agents and planners before stabilizing the single-agent loop and tool boundary.

**Fix:** return to the simplest architecture that meets requirements; centralize tool boundaries; define clear responsibilities; only split into multi-agent when evidence supports it.

**Prevention:** treat multi-agent as a later optimization and follow the guidance in `2.8_multi_agent_system.md`.

---

# 7. CASE STUDY: IMPLEMENTING AN AGENTIC PROCUREMENT ASSISTANT IN ENTERPRISE

This case study shows an end-to-end agent implementation where the agent can take actions. The goal is to demonstrate the full consulting method: framing, autonomy decisions, tool readiness, architecture, controls, evals, observability, and operating cadence. The details are realistic by design because enterprise agents fail when they are designed around ideal conditions.

## 7.1. Context: The Business Problem and Why "Chat" Was Not Enough

**Client context (example):** a global enterprise with 20k employees and decentralized procurement. Purchase requests are handled via email, spreadsheets, and a ticketing system. Cycle times are slow, policy compliance is inconsistent, and finance has limited visibility into why approvals are delayed.

The client tried a "procurement chatbot" that answered policy questions. It improved awareness but did not change outcomes because employees still needed to:

- collect missing fields (cost center, vendor, budget owner)
- interpret policy thresholds (approval chains, preferred vendors, contract requirements)
- open tickets and fill forms in the ERP
- route approvals and follow up

Leadership asked for an outcome: "reduce average purchase request cycle time by 30% without increasing compliance risk". That is an agent problem, not a Q&A problem.

## 7.2. Target Workflows and Autonomy Level (Start Conservative)

The team defines the first release around three workflows:

1. Create a purchase request (PR) draft with validated fields.
2. Route approvals based on policy and thresholds.
3. Create a requisition in the ERP after approvals (with controls).

The autonomy decision is explicit:

- Pilot target: **L2/L3** (execute read-only tools and low-risk writes; approvals for high-impact steps).
- Not allowed in pilot: creating purchase orders automatically, changing vendor bank details, or bypassing approvals.

This decision reduces program friction. Security and finance can approve L2/L3 with evidence; L4 is postponed until controls and metrics mature.

## 7.3. "P0 Failures" for This Agent (Written Before Building)

The program defines P0 failures as release blockers:

- unauthorized creation of a requisition or PO (wrong identity, wrong scope, wrong approval state)
- policy bypass (approvals skipped, thresholds ignored)
- data leakage (supplier PII, employee PII, contract terms) in responses or logs
- false confirmation ("PR created") when the ERP call failed or did not persist
- cross-tenant / cross-business-unit data exposure (wrong vendor details shown to the wrong group)

This definition shapes everything that follows: guardrails, tool gateway design, eval suites, alerts, and incident response.

## 7.4. Tool and Data Landscape (What Must Exist Before Agents Work)

The consulting team inventories systems and identifies what is missing:

- ERP API can create requisitions, but it has weak idempotency semantics.
- Ticketing system supports approvals, but approval state is not easily queryable.
- Vendor master data exists, but access is restricted by business unit.
- Policy documents exist in SharePoint, but are inconsistent and updated frequently.

The program makes an important decision: the agent will not call ERP APIs directly. All write actions will go through a **tool gateway** that provides:

- stable schemas and business validation
- idempotency keys
- audit logs with correlation ids
- policy checks (approval state, thresholds, identity)

This is the first "agentic architecture" lesson: tool readiness is a platform project.

## 7.5. Architecture Choice: A Constrained Agent Over a Deterministic Backbone

The team chooses a graph-controlled orchestration pattern:

- The deterministic graph defines the main stages (collect fields -> validate -> approvals -> create requisition -> confirm).
- The LLM is used inside narrow slots: intent classification, field extraction, clarification questions, and policy interpretation with citations.
- The tool gateway enforces permissions and state constraints.

The reason is enterprise reality: the workflow is high-stakes and auditable, but user input is messy. A pure free-form agent loop would be too expensive and too risky. A pure workflow would fail on ambiguity. The hybrid is the consulting-grade compromise.

Reference flow:

```text
User request
  -> classify intent + risk level
  -> build structured task state (fields + missing)
  -> retrieve policy excerpts (RAG) + require citations for thresholds
  -> propose next step (ask / validate / request approval / call tool)
  -> validate server-side and execute through tool gateway
  -> update state and repeat until done or escalated
```

## 7.6. Tool Registry and Schemas (The Backbone of Safety)

The team defines a tool registry (excerpt):

- `vendor_lookup(vendor_name, business_unit)` -> returns vendor_id, allowed flags
- `budget_check(cost_center, amount)` -> returns budget status and budget owner
- `create_pr_draft(fields...)` -> returns draft_id (reversible)
- `request_approval(draft_id, approver_role, reason)` -> returns approval_request_id
- `get_approval_status(approval_request_id)` -> returns approved/rejected/pending
- `submit_requisition(draft_id)` -> returns requisition_id (write, gated)

For each tool, the gateway enforces:

- required user roles and business unit scope
- schema validation and business rules (amount thresholds, allowed categories)
- idempotency keys for write actions
- strict error codes and structured outcomes

This is where the tool safety contract from `2.2_tool_calling.md` becomes concrete. The LLM never gets to "invent" a purchase requisition id. It must read it from tool outputs.

## 7.7. Guardrails and Approvals (Defense in Depth)

Guardrails are built into the workflow, not added as a post-filter:

- **Input checks:** detect obvious injection attempts and policy bypass requests ("skip approvals").
- **Retrieval checks:** enforce ABAC at retrieval time for policy documents by business unit and role (`2.1_rag.md`).
- **Tool checks:** server-side authorization and validation for every tool call; approvals required above thresholds.
- **Output checks:** redaction rules for PII and vendor data; citation requirement for policy thresholds.
- **Safe logging:** store ids and classifications, not raw payloads (see `2.6_observability_llomps.md`).

The approval model is explicit and risk-based:

- purchases under $1,000: agent can submit requisition after validations (L3 with policy checks)
- purchases $1,000-$10,000: manager approval required
- purchases above $10,000 or new vendor: procurement + finance approval required, plus HITL review

This design aligns the autonomy ladder with governance. The agent is useful early, and it can grow with evidence.

## 7.8. Delivery Timeline: 8 Weeks From POC to Pilot Readiness

The consulting team uses an 8-week plan that balances speed and control:

**Weeks 1-2 (foundation):**

- align on workflows, autonomy level, and P0 definitions
- build the tool registry and gateway scaffolding
- define the task state model and the orchestration graph
- define the telemetry policy (minimize, redact, retention, access)

**Weeks 3-4 (v0 agent):**

- implement intent classification and field extraction
- implement RAG for policy excerpts with citations
- enable read-only tools (vendor lookup, budget check)
- build the first end-to-end path: PR draft creation (reversible)

**Weeks 5-6 (controls + evidence):**

- implement approval workflow and thresholds in the gateway
- enforce truthful confirmation and error handling for timeouts
- build the v0 eval suite: must-pass and must-refuse scenarios (`2.5_evals.md`)
- implement tracing across steps and tools (`2.6_observability_llomps.md`)

**Weeks 7-8 (pilot readiness):**

- run canary rollout to procurement team first
- tune false positives (over-blocking) and improve clarification UX
- finalize runbooks and kill switches (disable submit tool, force HITL)
- produce the readiness pack for governance review (metrics + evidence + controls)

The deliverable is not only the agent. The deliverable is an operable capability with clear boundaries and evidence.

## 7.9. Evals: How the Team Proved the Agent Was Safe to Pilot

The team builds a scenario-based eval suite. Each scenario is scored on:

- correctness of extracted fields (and asking when missing)
- policy compliance (thresholds, approvals, role constraints)
- tool correctness (right tool, valid args, handling failures)
- safety constraints (no leakage, no bypass, no false confirmation)

Example scenario categories:

- normal PR creation with complete info (must pass)
- missing cost center (must ask, not guess)
- ambiguous vendor name (must disambiguate or lookup safely)
- new vendor onboarding request (must route to HITL, not execute)
- "skip approval" request (must refuse and explain policy)
- injection attempt inside a retrieved policy doc (must ignore and keep boundaries)

The team treats P0 failures as blockers. If the agent leaks data, bypasses approvals, or falsely confirms a write, the program does not pilot.

## 7.10. Observability: Diagnosing Failures by Boundary (Not by Blame)

The telemetry design makes every step traceable:

- request id, trace id, user role, business unit
- prompt version and orchestration config version
- tool gateway decision logs (allowed/blocked/approval requested) with reason codes
- tool execution outcomes and latency
- guardrail outcomes and redaction flags
- termination reason (completed, waiting approval, escalated, budget exceeded)

This enables a weekly operating forum where issues are discussed as systems problems:

- "we are failing budget_check for cost centers in region X because the tool is missing a mapping"
- "we have high clarification loops for vendor names because vendor_lookup does not support fuzzy matching"
- "we are over-blocking purchases that mention 'gift' due to a too-broad policy classifier"

Observability turns debates into backlogs.

## 7.11. A Real Incident and the Incident-to-Regression Loop

**Incident (pilot week 2):** the ERP API experiences intermittent timeouts. The agent calls `submit_requisition(draft_id)` and times out, then retries. Without idempotency, this would create duplicates.

What happened in this program:

1. The tool gateway generated an idempotency key based on `draft_id` and approval state.
2. The second call returned "already processed" with the same requisition_id.
3. The agent used truthful confirmation: it confirmed only after it received the requisition_id.
4. An alert fired for elevated ERP timeout rate, and the runbook recommended temporarily forcing HITL for submission.

How recurrence was prevented:

- A regression eval case was added: "ERP timeout + retry must not create duplicates and must not falsely confirm."
- A monitoring threshold was added: when ERP timeouts exceed X, automatically degrade to draft-only mode.

This is LLMOps for agents in practice: production failure becomes a durable test and an operating rule.

## 7.12. Results and Scaling Decision

After 6 weeks of pilot:

- average cycle time for low-risk purchases decreased because draft creation and field collection were automated
- policy compliance improved because the same checks were applied consistently
- procurement workload shifted from form-filling to exception handling (better leverage)

The scaling decision was made with evidence:

- expand to two additional business units (because ABAC and telemetry proved stable by slice)
- expand autonomy for low-risk categories only (because P0 remained zero and false confirmations were near zero)
- postpone autonomous PO creation (because the cost of error was too high and required additional controls)

The key takeaway is that "agentic AI" succeeded here because autonomy was staged, tools were made safe, and operating loops were explicit. The agent was not trusted because it sounded confident; it was trusted because it was measurable and controllable.

---

