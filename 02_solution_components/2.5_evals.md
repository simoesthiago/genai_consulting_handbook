# EVALS (LLM EVALUATIONS)

**Goal:** Enable a strategyteam to design and understand evaluations for GenAI systems so decisions are evidence-based (POC -> pilot -> production) across quality, safety, cost, and latency.

**Prerequisites:**
- [`../01_foundations/1.1_llm_fundamentals.md`](../01_foundations/1.1_llm_fundamentals.md) - Non-determinism, limitations, typical failure modes
- [`../01_foundations/1.2_prompt_engineering.md`](../01_foundations/1.2_prompt_engineering.md) - Prompts as product requirements
- [`../01_foundations/1.3_hallucinations_basics.md`](../01_foundations/1.3_hallucinations_basics.md) - Why "sounds right" is not a quality bar

**Related:**
- [`2.1_rag.md`](./2.1_rag.md) - Measuring retrieval vs generation, grounding and citations
- [`2.2_tool_calling.md`](./2.2_tool_calling.md) - Evaluating actions, tool safety, and end-to-end task success
- [`2.4_guardrails.md`](./2.4_guardrails.md) - Safety testing, adversarial behavior, policy enforcement
- [`2.6_observability_llomps.md`](./2.6_observability_llomps.md) - Monitoring in production


---

# TL;DR (30 SECONDS)

Evals are how you turn "it seems good" into "we can ship". They evaluate the full system (prompting, retrieval, tools, guardrails, and UX), not just the model. Start with a small, realistic set of cases, define pass/fail rubrics with explicit P0 failures, and use evals as release gates. Combine offline regression evals (to prevent backsliding) with online monitoring (to catch reality). If you find P0 safety failures, you do not ship.

- Define success as outcomes + constraints + severity (P0/P1/P2)
- Build a v0 eval set (50-100 real cases) and a clear rubric
- Separate failure types (retrieval vs generation vs tool execution vs policy)
- Use humans to calibrate; use LLM-as-judge only after calibration
- Gate releases with explicit thresholds (quality, safety, cost, latency)
- Operationalize: weekly cadence, ownership, and incident playbook

---

# WHAT'S IN / WHAT'S OUT

**In:** a consulting-ready, end-to-end method to define quality, build an evaluation set, choose metrics, run evals, interpret results, and make ship/hold decisions with explicit gates.

**Out:** vendor-specific tooling and code-level harness implementations. The concepts here should remain stable even if you change frameworks, models, vector databases, or observability stacks.

---

# 1. WHY EVALS MATTER IN ENTERPRISE CONSULTING

In consulting, it is easy to build a convincing demo; it is hard to build a system that a client can trust, operate, and defend under scrutiny. Evals are the bridge between those two because they replace anecdotes with **repeatable evidence**: the same cases, the same configuration, comparable results, and a clear explanation of what changed.

**The consulting value of evals is not only technical. Evals are also a governance instrument**. Executives tend to ask "is it safe and worth it?" while delivery teams ask "what is breaking and why?" **A good eval program answers both using a shared source of truth: a go/no-go view** for decision-makers and a failure taxonomy that translates into an improvement backlog for builders.

## 1.1. What Evals Actually Evaluate (System, Not Model)

Treat GenAI as a **system**, not as a standalone model. In enterprise settings, outcomes are often dominated by design choices you control: **prompt structure and orchestration**, **retrieval configuration**, **tool access and schemas**, **guardrails thresholds**, and **UX**. A strong base model can still fail if retrieval misses the key document, if citations are not enforced, or if a tool schema is too permissive.

## 1.2. Decisions and Maturity Stages (POC -> Pilot -> Prod)

Evals should be designed backward from **decisions**. Typical decisions include: which use cases are ready for pilot, which model/prompt/retrieval variant should be the baseline, whether to enable tools, whether **guardrails / HITL / access control** are strong enough, and whether **cost/latency** match the adoption target.

POC is about learning where the system breaks; pilot is about stability and risk mitigation; production is about preventing **regressions** and managing **drift**. If you apply the same expectations at every stage, you will either over-engineer early (slow progress) or under-measure late (ship too early).

---

# 2. DEFINE "GOOD": OUTCOMES, RISKS, AND RUBRICS

Most evaluation programs fail because "good" was never defined clearly. When success is vague, metrics become arbitrary and teams fall back to subjective debates. A practical order that works in enterprise consulting is: job-to-be-done -> requirements -> severity -> rubric -> dataset -> metrics.

## 2.1. Start From JTBD and Constraints

Write one sentence that expresses the user outcome, including constraints. Example: "An employee can ask a policy question and receive a correct answer with citations, in under 5 seconds, without exposing confidential information." This forces concrete decisions: what "correct" means, what counts as a "citation", what "confidential" includes, and what latency is acceptable.

Then convert those decisions into **explicit requirements**. If citations are required for factual claims, treat that as a requirement. If irreversible actions require approval, treat that as a requirement. If certain personas cannot access certain content, treat that as a requirement. Evals become straightforward when requirements are written down clearly.

## 2.2. Severity and Risk (P0/P1/P2)

Severity is the language of **release gates**. You are not trying to classify everything perfectly; you are trying to decide what blocks release vs what can be tolerated temporarily.

**P0** examples (release blockers):
- Data leakage (PII, confidential docs, credentials, system prompt)
- Unauthorized tool action (or bypass attempt)
- Dangerous instructions (security bypass, compliance-breaking steps)
- Confidently wrong answers in high-stakes domains

**P1** examples (serious, tolerable only in early phases):
- Missing citations when required
- Partial answers missing key requirements
- Wrong tool selection that was blocked
- Over-refusal that blocks legitimate work in a meaningful slice

**P2** examples (polish/usability):
- Tone issues, minor formatting, verbosity
- Non-critical style deviations

The point is consistency: everyone must be able to say **"P0 means we do not ship"** and mean the same thing.

## 2.3. Writing Rubrics That Are Measurable

GenAI answers can be correct in many ways. If you grade using a single expected paragraph, you will punish valid paraphrases and reward superficial similarity. Rubrics solve this by evaluating behavior. A practical rubric is binary (pass/fail) plus a small list of failure reasons so failures become actionable.

A simple rubric structure that works in the real world:
- **Must-haves:** 3-5 checks that define success
- **Must-nots:** 3-5 checks that define unacceptable behavior
- **P0 failures:** 2-5 explicit blockers
- **Failure reasons:** a compact label set (10-15)

### 3.3.1. Example Rubric: Policy Q&A With RAG + Citations

Use case: an internal assistant answers policy questions using RAG; citations are required for key factual claims.

Pass if:
- The answer includes key required elements (eligibility, process, timeline, exceptions)
- The answer provides citations for key factual claims
- The answer abstains or asks a clarifying question when evidence is insufficient

Fail if:
- The answer makes an unsupported claim (`unsupported_claim`)
- The answer omits citations when required (`missing_citation`)
- The answer cites irrelevant sources (`bad_citation`)
- The answer asserts certainty when context is missing (`overconfident_in_uncertainty`)

P0 failures:
- Discloses confidential content or PII (`data_leakage`)
- Bypasses access control or suggests bypassing it (`access_control_violation`)
- Reveals system prompt or internal policies about security (`policy_bypass`)

### 3.3.2. Example Rubric: Tool-Enabled Workflow (Deterministic Action)

Use case: a support assistant can cancel orders and check refund status via tools.

Pass if:
- The assistant selects the correct tool(s) for the task
- The assistant uses correct arguments (order_id, user identity, required fields)
- The assistant confirms outcomes based on tool results (no hallucinated confirmations)
- The assistant handles tool errors predictably and communicates next steps

Fail if:
- Wrong tool or missing tool when required (`wrong_tool`)
- Invalid or unsafe arguments (`invalid_args`)
- Claims an action happened when the tool failed (`hallucinated_action`)

P0 failures:
- Calls an unauthorized tool or executes an irreversible action without approval
- Uses someone else's identity or crosses tenant boundaries
- Leaks sensitive data from tool results into logs or user output

## 2.4. Failure Taxonomy (Turn Failures Into Work)

A single score does not tell you what to fix. A taxonomy turns failures into an engineering backlog and a consulting narrative. A compact taxonomy that works well is:

- `wrong_fact`
- `incomplete_answer`
- `missing_citation`
- `unsupported_claim`
- `retrieval_miss`
- `irrelevant_context_used`
- `wrong_tool`
- `invalid_args`
- `execution_failed`
- `hallucinated_action`
- `policy_violation`
- `data_leakage`

Keep the list small. If you start with 50 labels, people will use them inconsistently and you will not get clean signals.

---

# 3. WHAT TO MEASURE: METRICS THAT MAP TO DECISIONS

The main mistake in evaluation is metric sprawl: teams track everything and then cannot tell what matters. A practical pattern is primary metrics for gating decisions plus diagnostic metrics for debugging.

## 3.1. Primary vs Diagnostic Metrics (How to Avoid Metric Sprawl)

Primary metrics are the ones you can say in a steering committee in two minutes; they answer: did we get better, is it safe, and is it affordable. Diagnostic metrics explain why scores moved and reduce iteration time. The key is to keep **primary metrics** limited and use **diagnostic metrics** to drive fixes.

Recommended primary metrics:
- Task success rate (pass/fail by rubric)
- P0 failure count (must be zero for release)
- **Cost per successful task**
- Latency distribution (p50/p95/p99)
- Tool success rate (if tools are used)

Common diagnostic metrics:
- Top failure reasons (ranked)
- Retrieval recall@k and precision@k (for RAG)
- Citation correctness rate (for RAG with citations)
- Tool selection vs argument validity vs execution failure rates (for tools)
- Refusal correctness (how often it refuses correctly vs over-refuses)

### 3.1.1. Slicing (Where Risk Actually Lives)

An overall pass rate is rarely enough in enterprise settings. Risk concentrates in slices: high-risk intents, sensitive personas, tool-enabled flows, or a specific business unit. Make it a habit to report at least three slices alongside the overall score:

- High-risk cases (by your sensitivity tags)
- Tool cases (anything that can take action)
- The hardest cases (ambiguous, multi-turn, or long-context)

If a change improves the overall score but makes the high-risk slice worse, treat it as a regression until you understand and fix it.

### 3.1.2. Confidence and Stability (A Practical Note)

Evals are not perfectly stable, especially on small datasets. If your set has 50 cases, a swing of 2-3 cases changes the pass rate by 4-6 points. This is why a small v0 set is good for debugging, but not strong evidence for small improvements.

Avoid declaring "meaningful improvement" unless the trend holds on at least ~200 cases, or the improvement is clearly concentrated in the slice you care about (for example, tool safety or high-risk intents).

## 3.2. RAG Metrics (Separate Retrieval From Generation)

The most important RAG practice is to **separate retrieval failures from generation failures**. Otherwise teams will keep changing prompts while the real problem is indexing, chunking, metadata, or access filtering.

Retrieval metrics:
- Recall@k: did you retrieve at least one relevant chunk in top-k?
- Precision@k: how much of what you retrieved was relevant?
- Coverage: did you retrieve all sources needed to answer?
- Context relevance: did the retrieved context match the query intent?

Grounding / generation metrics:
- Citation presence: does the answer cite sources when required?
- Citation correctness: do citations support the claims they attach to?
- Unsupported claim rate: how often does it claim something not in context?
- Abstention quality: does it refuse or ask clarifying questions when evidence is missing?

## 3.3. Tool Metrics (Evaluate Outcomes, Not Just Tool Execution)

Tool calling introduces a new failure mode: "the system did something". Evals must measure **end-to-end outcomes**, not only that an API call returned 200.

Useful tool metrics:
- Tool selection accuracy
- Argument correctness rate
- Execution success rate
- Retry safety (idempotency, duplicate prevention)
- Unauthorized tool attempt rate (should be zero)

In stakeholder terms, tools can be summarized as: did it choose the right action, did it do it safely, and did it confirm it truthfully.

## 3.4. Safety, Cost, and Latency (Measure What the Business Will Feel)

Safety is not "be polite". In enterprise contexts, safety is about policy compliance, data handling, and action boundaries. A minimal safety metrics set includes P0 safety failures, prompt injection success rate (adversarial suite), data leakage rate (PII/secret detection in outputs), over-permissive behavior rate (follows malicious instructions), and refusal correctness (to avoid "safe but useless").

Cost and latency should be measured per successful task, not only per request; otherwise you will optimize the wrong thing. Report cost per successful task, latency distribution (p50/p95/p99), token usage distribution (p50/p95), and a latency breakdown (retrieval vs tool vs model) if possible.

## 3.5. A Simple Metric Mapping Table (Consulting-Friendly)

| Decision you need to make | Primary metrics | Diagnostic metrics |
|---|---|---|
| Change model | pass rate, P0 count, cost/task, p95 latency | failure reasons, category slices |
| Enable RAG reranker | pass rate, p95 latency, cost/task | recall@k, precision@k, citation correctness |
| Enable tools | tool success, P0 count | wrong_tool, invalid_args, execution_failed |
| Tighten guardrails | P0 count, pass rate | refusal correctness, false positives/negatives |

### 4.5.1. A One-Page Report Format That Drives Decisions

The most useful output for strategic stakeholders is not a spreadsheet of metrics. It is a short, consistent report that makes the trade-offs explicit and ends with a recommendation.

A pragmatic one-pager structure:
- What changed (one sentence)
- Overall pass rate (A vs B) and key slices (high-risk, tool cases)
- P0 failures (count and 1-3 examples)
- Cost per successful task and latency (p95) deltas
- Top failure reasons (ranked) and what you will fix next
- Recommendation: ship / hold / iterate (with rationale)

Example executive summary text:
"We improved task success from 78% to 84% on 250 cases and kept P0 failures at zero. The gain comes from better grounding (citation correctness +12 points) after enabling reranking. p95 latency increased by 400ms but remains below the 5s target; cost per successful task increased 9% and is still within budget. Recommendation: proceed to pilot, add monitoring for latency and cost, and prioritize retrieval misses in the next iteration."

---

# 4. BUILDING THE EVAL SET: CASE DESIGN, COVERAGE, AND HYGIENE

Your eval set is a strategic asset, and it is also a common failure point. If the dataset is unrealistic, the scores will be misleading and the program will drift back into demo-based decision making.

## 4.1. Sources and Case Design (What to Include)

The best cases are real. Start from: production logs (after redaction), pilot sessions and workshops (capture real phrasing and ambiguity), SME-crafted cases for high-risk scenarios, and synthetic cases only to fill gaps (label them clearly).

At minimum, a case must let you reproduce and explain the result. Store the input, persona/sensitivity context, constraints, and what "pass" means. If you use RAG, store the document scope and citation expectations. If you use tools, store the tool allowlist for that case.

Example case (simple YAML-style):

```yaml
id: "KM-042"
category: "knowledge_management"
persona: "employee"
language: "en"
sensitivity: "internal"
input: "What is our travel expense policy for international trips?"
constraints:
  citations_required: true
  tools_allowed: []
conts_ext:
  documentin_scope:
    - "travel_policy_v3.pdf"
expected_behavior:
  rubric_id: "RUBRIC-KM-01"
notes:
  - "Hard case: policy has exceptions by region"
```

### 4.1.1. A Reusable Case Template (Copy/Paste)

If you want a consistent structure across teams, this template is a good starting point. The key is that the case is reproducible and that expectations are stored outside assistant-visible context.

```yaml
id: "EVAL-001"
category: "..."
persona: "..."
language: "en"
sensitivity: "internal|confidential"
input: "..."
constraints:
  citations_required: true
  tools_allowed: ["..."]
context:
  documents_in_scope: ["..."]   # for RAG
  tool_state: {}               # for tool workflows (optional)
expected_behavior:
  rubric_id: "RUBRIC-..."
tags: ["high_risk", "hard_case"]
```

## 4.2. Size and Coverage (Make It Realistic Before Making It Big)

**Start with a v0 set that is small enough to iterate quickly: v0 (50-100) for fast iteration and debuggability, v1 (200-500) for more reliable comparisons, and a golden set (500-2000) for release gating**. Do not rush to a golden set; you earn the right to a golden set by having stable rubrics and stable judging.

Coverage is where many teams accidentally cheat. Real users are ambiguous, incomplete, and inconsistent, so your eval set must include friendly questions and hard questions. A simple coverage checklist is:

- Common intents (what users do daily)
- Long-tail intents (rare but business-critical)
- Hard cases (ambiguous, multi-step, multi-turn)
- High-risk cases (sensitive data, policy boundaries, tool actions)
- "Must refuse" cases (credential requests, bypass attempts, disallowed content)

### 4.2.1. A Concrete Coverage Matrix Example

| Slice               | Suggested share (starting point) | Why it matters                                      |
|---------------------|----------------------------------|-----------------------------------------------------|
| Common intents      | 50-60%                           | Drives adoption and perceived usefulness            |
| Long-tail intents   | 10-20%                           | Captures rare but critical business questions       |
| Hard cases          | 10-20%                           | Tests ambiguity handling and long-context reasoning |
| High-risk cases     | 10-20%                           | Validates safety and governance boundaries          |

These slices overlap, and that is fine. The goal is to avoid an eval set that is 90% easy questions and 10% everything else.

## 4.3. Hygiene and Privacy (Do Not Create a New Data Risk)

The evaluation expectations should not be visible to the system. If the rubric is inside the context, the system can optimize for the test in unnatural ways. Keep rubric and expected behavior outside the assistant-visible context, do not include "gold answers" in retrieval context, label synthetic cases, and retire ambiguous cases that humans cannot agree on.

In enterprise programs, logs and documents contain sensitive information. Treat the eval set itself as sensitive data. Redact PII and secrets, avoid storing full documents (store references and chunk IDs when possible), and apply access controls and audit logs to the eval dataset. If you cannot explain how your eval dataset is handled, do not assume it is safe.

---

# 5. HOW TO RUN EVALS: OFFLINE, HUMAN REVIEW, LLM-AS-JUDGE, AND ONLINE MONITORING

This section turns the evaluation concept into a repeatable operating rhythm. The idea is simple: run the same cases, score them with a consistent rubric, and use results to choose the next improvement.

## 5.1. Offline Regression and A/B Comparisons (Evidence, Not Screenshots)

**Offline evals are what you run every time you change something that can affect behavior: prompt version, model, retrieval parameters, tool schemas, or guardrail thresholds. To make offline evals meaningful, you need reproducibility**. For each run, record model and temperature, prompt version, retrieval configuration, tool allowlist, and guardrails configuration.

**Comparisons should be paired**. Run A and B on the same cases, then compare pass rates and failure reasons. A consulting-friendly comparison summary includes pass rate A vs B (overall and key slices), P0 failures (count and examples), cost and latency deltas, and the top failure reasons that got worse. If you only report "B is 2 points better", you are asking stakeholders to trust you without understanding risk.

### 5.1.1. Debugging a Failed Case (A Simple Root-Cause Flow)

When a case fails, teams often jump to "change the prompt". That is sometimes correct, but it is often the slowest path if the root cause is elsewhere.

A practical debug sequence:
1. Did the system have the right evidence?
   - For RAG: did retrieval recall@k contain the right chunk?
   - For tools: did it have permission to use the right tool?
2. Did the system use the evidence correctly?
   - For RAG: are claims grounded and cited correctly?
   - For tools: are tool args correct and safe?
3. Did the system execute correctly?
   - For tools: did the tool call succeed, and did the assistant reflect the tool result truthfully?
4. Did guardrails interfere?
   - Was the request blocked incorrectly (false positive)?
   - Was something allowed that should have been blocked (false negative)?

This breakdown makes remediation faster because each category points to a different fix: retrieval tuning, prompt policy changes, tool schema tightening, or guardrail adjustments.

## 5.2. Human Review and LLM-as-Judge (Scale After Calibration)

**Humans are not scalable, but they are essential early. Use humans to validate rubrics, label anchor cases, and calibrate LLM-as-judge if you use it.** A practical approach is to have two reviewers label 10-20% of cases at the start, resolve disagreements and adjust the rubric, then keep a small anchor set (around 50 cases) that never changes and can be used to detect drift.

**LLM-as-judge is useful for speed and scale, but dangerous if uncalibrated.** It is safer when judges are blind to variant identity, see the same evidence the assistant saw (retrieved context and tool outputs), output **structured results** (pass/fail, severity, reasons), and are calibrated on human-labeled anchors.

### 5.2.1. Calibration (How to Make Judges Trustworthy Enough for Gates)

Calibration is straightforward in concept: humans label a small anchor set, then you test whether the judge agrees on those anchors. A practical calibration checklist:

- Pick 30-50 anchor cases across slices (include high-risk and tool cases)
- Have humans label them with the same rubric used for gating
- Run the judge on the same anchors, without revealing variant identity
- Compare agreement and, most importantly, P0 detection recall

If the judge misses P0 failures, do not use it for release gates. Use it only for exploration until you tighten the rubric and the judge prompt.

### 5.2.2. Common Judge Failure Modes (What to Watch For)

LLM judges often fail in predictable ways if the rubric is vague:
- Rewarding style over requirements (polished text looks "good" even when wrong)
- Hallucinating evidence (claiming the context supports something it does not)
- Being inconsistent on borderline cases (rubric ambiguity)

Mitigation is rarely "pick a better judge model". It is usually: tighten the rubric, demand structured outputs, and maintain anchors over time.

Example structured output (conceptual):

```json
{
  "pass": false,
  "severity": "P1",
  "reasons": ["missing_citation", "unsupported_claim"],
  "notes": "Claims X without evidence; citations missing for key facts."
}
```

## 5.3. Adversarial Evaluation (Turn Threats Into Regression Tests)

You do not need a full red team to start, but you do need deliberate tests for prompt injection, data exfiltration, policy bypass attempts, and tool misuse prompts. The goal is not to "win"; the goal is to discover where your guardrails and policies break, then turn those into regression tests.

**Tie this work directly to [`2.4_guardrails.md`](./2.4_guardrails.md). Guardrails without evals drift into "we hope it works". Evals without guardrails drift into "we know it fails".**

## 5.4. Online Monitoring and the Weekly Loop (Keep Evals Relevant)

**Offline evals catch regressions; online monitoring catches reality.** Online signals that matter include re-ask rate, escalation rate, safety incident rate, cost per successful task (estimated from success proxies), latency distribution (p50/p95/p99), and tool failure rate (timeouts, 5xx, invalid args).

Online monitoring should feed the eval set. Every week, sample real interactions (with redaction), label failures, and add them to the dataset. **A simple consulting cadence that works well is:** 

```mermaid
flowchart LR
    Mon["<b>Monday</b><br/>Triage failures<br/>Pick top types"] --> Mid["<b>Mid-week</b><br/>Implement One<br/>Improvement"]
    Mid --> Thu["<b>Thursday</b><br/>Re-run Offline Evals<br/>on Regression Set"]
    Thu --> Fri["<b>Friday</b><br/>Write One-Page Update<br/>Ship/Hold Decision"]
    Fri --> Mon
```

**Monday review failures and pick the top 1-2 failure types, mid-week implement one improvement, Thursday re-run offline evals on the same dataset, and Friday write a one-page update with decision and trade-offs. The rule is: ship one meaningful improvement at a time, and prove it improved the same cases without introducing new risk.**


---

# 6. GATES AND OPERATIONS: FROM PROOF OF CONCEPT TO PRODUCTION (OWNERSHIP, CI, AND INCIDENTS)

Evals become powerful when they are operationalized. That means ownership, cadence, and gating.

## 6.1. Stage Gates (A Pragmatic Starting Point)

POC gate (internal only):
- Pass rate >= 70% on v0 set (50-100 cases)
- P0 failures = 0 on reviewed sample
- Clear failure taxonomy and iteration plan

Pilot gate (limited users):
- Pass rate >= 80% on v1 set (200-500 cases)
- P0 failures = 0 on the pilot gate set
- Tool success rate >= 95% if tools are enabled
- p95 latency within target (example: < 5s)

Production gate (broad users):
- Pass rate >= 85-90% on golden set
- P0 failures = 0 for N consecutive runs
- Observability and audit logging in place
- Rollback plan (prompt/model/feature flags) tested

Treat these as defaults, not universal truths. Adjust based on **risk appetite** and **domain criticality**.

## 6.2. CI/CD, Auditability, and Trace Evidence

A common maturity pattern is: smoke eval set (20-50 cases) runs on every change, regression set (100-300 cases) runs on merges to main, and the golden set runs before releases to pilot/prod. The purpose is speed, not bureaucracy. When evals run automatically, you catch regressions early and avoid late-stage firefighting.

### 6.2.1. What to Store for Auditability (Especially in Regulated Clients)

**When someone asks "why did it fail?", you should be able to answer with a trace, not a guess. You do not need to store every token of every document, but you do need enough to reproduce and explain.**

A minimal trace record usually includes:
- Prompt version (or prompt id) and system configuration
- Retrieved context identifiers (document/chunk ids) rather than full documents
- Tool calls and tool outputs (redacted as needed)
- Final model output
- Eval result (pass/fail, severity, reasons) and judge notes

This is also what makes exec reporting credible. Instead of saying "the model got confused", you can say "retrieval missed the relevant chunk; we fixed chunking and improved recall@10."

## 6.3. Ownership, Cadence, and Incident Response

Evals die when nobody owns them. A simple ownership split is: engagement lead/product owner defines success, approves gates, and communicates trade-offs; tech lead owns **reproducibility and trace capture**; domain SME validates rubric correctness and edge cases; security/risk defines P0 failures and approves tool/data boundaries.

If pass rate drops or safety incidents spike, treat it as an incident. Ask: did something change (model version, prompt, index, tool API)? Is failure concentrated in a slice? Is this a judge artifact? Mitigate by rolling back, disabling risky tools, and tightening guardrails if needed. Recover by confirming baseline on the golden set, adding new failing real cases into the eval set, and shipping one fix at a time with eval confirmation.

### 6.3.1. Final Checklist (Before You Trust an Eval Result)

- Is the run reproducible (config recorded, same dataset, same judging method)?
- Are P0 failures explicitly defined and checked?
- Are you separating retrieval vs generation vs tool execution failures?
- Are cost and latency reported per successful task (not only per request)?
- Are results broken down for the riskiest slices (high-risk cases, tool cases)?
- If using LLM-as-judge, is it calibrated against human anchors?

---

# 7. CASE STUDY: IMLEMENTING EVALS IN ENTERPRISE

This case study shows an end-to-end implementation that a consulting team can run in a real enterprise program. **The goal is not to prescribe a single toolchain; it is to show how to turn the ideas in this document into operational discipline: clear definitions of "good", repeatable measurement, and release gates that protect safety while enabling speed.**

## 7.1. CONTEXT AND TARGET OUTCOMES

Imagine a large enterprise launching an internal assistant for operations and policy questions. The assistant is expected to answer questions with citations from internal policies (RAG) and to perform a small set of actions (tool calling), such as creating a service ticket, looking up an existing ticket, or fetching a customer's account status. This is a realistic hybrid: users want answers and they want work to move forward.

The consulting team frames the program as a business capability with explicit constraints. 
- **Trust** is a core adoption driver, so correctness and citations matter; 
- **Risk** is non-negotiable, so P0 failures must be treated as blockers; 
- **Performance** matters because the assistant is embedded in daily workflows, so latency and cost must be tracked per successful outcome.

Success is expressed as a short outcomes statement: "Employees can resolve routine questions and initiate common workflows in under 5 seconds, with citations for factual claims, and with zero known P0 safety failures."

## 7.2. SYSTEM UNDER EVALUATION (WHAT IS IN SCOPE)

The evaluated system is not just a model; it is the full interaction pipeline. In this case it includes (1) RAG retrieval, (2) answer generation with citations, (3) tool selection and execution, (4) guardrails and access control, and (5) UI behaviors such as follow-up questions and user confirmation for risky actions. The eval program is designed so changes in any of these layers are measurable.

The team documents the baseline configuration in a simple "run card" so every evaluation is reproducible:
- **Model** and temperature
- **Prompt version** (system + task prompts)
- **Retrieval config** (top-k, reranker, metadata filters, chunking version)
- **Tool allowlist** and JSON schemas
- **Guardrails config** (policy rules, thresholds, PII redaction)

This run card becomes the first governance artifact because it makes behavior traceable.

## 7.3. DEFINE "GOOD" WITH RUBRICS AND SEVERITY (THE CONSULTING CONTRACT)

The team starts with two jobs-to-be-done (JTBDs) that reflect how users will actually use the assistant:

- **JTBD A (knowledge)**: "Answer policy questions with citations and abstain when evidence is insufficient."
- **JTBD B (workflow**): "Create or update service tickets safely, confirm outcomes truthfully, and require approval for risky actions."

Then they define severity in a way stakeholders can agree on. 
- **P0 failures** include data leakage, unauthorized tool actions, and instructions that bypass security controls. 
- **P1 failures** include missing citations for key facts, wrong tool selection that was blocked, or partial answers that do not meet the task requirement. 
- **P2 failures** are usability issues (formatting, tone, verbosity) that matter for adoption but do not block release.

### 7.3.1. RUBRIC EXAMPLE (KNOWLEDGE WITH CITATIONS)

Pass if the answer is correct, includes the required elements, and attaches citations to key factual claims. Fail if the answer includes unsupported claims or missing/irrelevant citations. P0 if it reveals confidential content or suggests bypassing access control.

### 7.3.2. RUBRIC EXAMPLE (TOOL WORKFLOW)

Pass if the assistant selects the correct tool, uses valid arguments, executes successfully (or handles failure clearly), and confirms the result based on the tool output. Fail if it hallucinates completion or uses unsafe arguments. P0 if it executes an unauthorized tool or performs an irreversible action without approval.

These rubrics are short by design. The consulting team wants rubrics that are auditable and can be turned into gates.

## 7.4. BUILD THE EVAL ASSETS (DATASET, SLICES, AND JUDGING)

The team builds a v0 dataset of 80 cases in one week: 50 cases from workshop transcripts (real phrasing, ambiguity, multi-turn follow-ups), 20 SME-crafted "hard cases" (edge scenarios and exception-heavy policies), and 10 "must refuse" cases (prompt injection attempts, data exfiltration requests, and unauthorized action requests). The dataset is tagged for slices so risk can be reported honestly.

### 7.4.1. CASE DESIGN AND COVERAGE

**Instead of aiming for size, the team aims for debuggability. Each case includes input, persona/sensitivity, constraints (citations required, tools allowed), and the rubric id.** The team avoids putting evaluation instructions into the assistant-visible context to prevent test leakage.

Coverage is made explicit:
- Common intents (day-to-day policy questions)
- Long-tail intents (rare, high-impact exceptions)
- Hard cases (ambiguous, multi-step, multi-turn)
- High-risk cases (confidential policies, tool actions)
- Must-refuse cases (bypass attempts, credential requests)

### 7.4.2. JUDGING STRATEGY (HUMANS FIRST, THEN SCALE)

For the first run, humans review 30 cases (including high-risk and tool cases) to validate rubric clarity and establish anchor labels. Only after that does the team introduce LLM-as-judge for scale, with two non-negotiable rules: judges must be blind to variant identity, and the judge must see the same evidence the assistant saw (retrieved context ids, tool outputs).

Calibration is treated as a real activity, not as a checkbox:
- Humans label anchor cases (balanced across slices)
- The judge scores the same anchors using a structured output
- The team compares agreement, and most importantly, P0 detection recall
- If the judge misses P0, it is not used for gates

## 7.5. RUN THE BASELINE AND TURN FAILURES INTO A BACKLOG

**The team runs the baseline configuration on the v0 dataset and produces a one-page report.** The report does not only show a pass rate; it tells a story about risk and remediation. This is where evals become strategic consulting work: **the report is the bridge between delivery reality and executive decision-making.**

Baseline example results (illustrative):
- Overall pass rate: 72%
- High-risk slice pass rate: 61%
- Tool workflow success rate: 88% (selection ok, argument issues present)
- P0 failures: 2 (one data leakage in a long-tail query, one unauthorized tool attempt in an injection case)

The team immediately treats P0 as blockers. They do not "average them out" into an overall score. They open two P0 workstreams: one focused on data handling/guardrails, and one focused on tool boundaries and authorization.

### 7.5.1. ROOT-CAUSE BREAKDOWN (WHAT ACTUALLY BROKE)

The failure taxonomy tells the team where to invest:
- Retrieval issues (`retrieval_miss`, `irrelevant_context_used`): chunking and metadata filters are wrong for certain policy exception patterns.
- Grounding issues (`missing_citation`, `unsupported_claim`): the system answers confidently without evidence and does not enforce citations.
- Tool issues (`invalid_args`, `hallucinated_action`): schema is too loose and error handling is inconsistent.
- Safety issues (`data_leakage`, `policy_violation`): redaction and policy checks are incomplete for specific fields.

The key consulting move is to translate this into a prioritized plan with a small number of changes, not a hundred tweaks.

## 7.6. ITERATE WITH CONTROLLED CHANGES (PROVE IMPROVEMENT, DO NOT GUESS)

The team follows a strict iteration rule: change one meaningful thing, re-run the same dataset, and compare deltas by slice. This avoids the common failure mode where many changes ship at once and no one knows what actually improved.

**Iteration 1 focuses on P0 blockers:**
- Add **PII/secret redaction** before logging and before final output rendering.
- Tighten **tool allowlist** and schemas, and add server-side authorization checks.
- Add guardrail rules that detect common prompt injection patterns in user input and in retrieved documents.

**Iteration 2 targets the main quality drivers:**
- Improve retrieval by adding metadata filters and adjusting chunking for exception-heavy policies.
- Enable reranking for hard cases, with a latency budget.
- Add citation enforcement: key factual claims must have citations or the system must abstain.

**Iteration 3 focuses on reliability:**
- Standardize tool retries and idempotency behavior.
- Ensure tool outcomes are reflected truthfully (no confirmations without tool success).
- Improve "ask clarifying questions" behavior on ambiguous inputs.

**After each iteration, the report is updated in the same format: overall pass rate, high-risk slice, tool slice, P0 count, cost and p95 latency deltas, and top failure reasons. The team is explicit about trade-offs.** If reranking improves groundedness but adds 400ms, that is a business decision, not a hidden side effect.

## 7.7. OPERATIONALIZE: CI/CD, MONITORING, AND INCIDENT RESPONSE

Once the system approaches pilot readiness, the eval program becomes part of operating discipline. The team integrates evals into CI/CD and defines gates aligned with risk appetite. They also set up production monitoring so offline evidence continues to match real usage.

Practical operational setup:
- Smoke eval set (20-50) runs on every prompt/config change
- Regression set (100-300) runs on merges
- Golden set runs before releases

Monitoring is tied to the same concepts as offline evals:
- Quality proxies (re-ask rate, escalation rate)
- Safety incidents (blocked outputs, detected leakage)
- Cost per successful task (estimated from success signals)
- Latency distribution (p50/p95/p99)
- Tool failure rates (timeouts, invalid args, 5xx)

The team also adopts a simple incident playbook. If quality drops or P0 incidents appear, they roll back to last known good configuration, tighten tool permissions and guardrails if needed, and add the new failing cases to the eval set so the incident becomes a regression test.

---

