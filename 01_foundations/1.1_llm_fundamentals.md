# LLM FUNDAMENTALS

**Goal:** Build a correct mental model of Large Language Models (LLMs) for enterprise decision-making, so consulting teams can choose the right patterns, set realistic expectations, and deliver systems that are **reliable, safe, and measurable**.

**Prerequisites:**
- None (this is the starting point).

**Related:**
- [`1.2_prompt_engineering.md`](./1.2_prompt_engineering.md) - Turning intent into a behavior contract (role, task, constraints, format)
- [`1.3_hallucinations_basics.md`](./1.3_hallucinations_basics.md) - Why models can be confidently wrong and how to mitigate it
- [`../02_solution_components/2.1_rag.md`](../02_solution_components/2.1_rag.md) - Grounding answers in enterprise knowledge with retrieval and citations
- [`../02_solution_components/2.2_tool_calling.md`](../02_solution_components/2.2_tool_calling.md) - Using tools safely to fetch real-time data and take actions
- [`../02_solution_components/2.4_guardrails.md`](../02_solution_components/2.4_guardrails.md) - Enforcing safety beyond what prompts can guarantee
- [`../02_solution_components/2.5_evals.md`](../02_solution_components/2.5_evals.md) - Measuring quality/safety/cost/latency and gating releases

---

# TL;DR (30 SECONDS)

An LLM is a **probabilistic prediction engine** that generates text by predicting the next token based on the tokens it can "see" in its context window. It is not a database, not a search engine, and not an authority on truth. In enterprise programs, most failures come from treating the model like it "knows" things or will "figure it out". Successful teams treat LLMs as components in a system: they control inputs (prompts, retrieved context, tool outputs), enforce boundaries (guardrails), and measure outcomes (evals and monitoring).

- LLMs generate **plausible text**, not guaranteed facts; trust requires grounding + measurement
- Tokens drive **cost** and **latency**; context size is a real constraint, not an inconvenience
- Non-determinism is normal; consistency comes from **contracts** (prompts + schemas) and **gates** (evals)
- Use RAG when the answer must be grounded in enterprise sources; use tools for real-time state/actions
- Safety is not "tone": it is data handling, authorization, and policy enforcement

---

# WHAT'S IN / WHAT'S OUT

**In:** an operational mental model (tokens, context, prediction), enterprise-relevant trade-offs (quality, cost, latency, safety), and a delivery playbook for how LLM fundamentals translate into architecture choices, governance, and measurement.

**Out:** deep training math, transformer implementation details, or model-internals research. Those rarely change strategic decisions in enterprise delivery.

---

# 1. WHY LLM FUNDAMENTALS MATTER IN ENTERPRISE CONSULTING

Most GenAI consulting projects do not fail because the model is "bad". They fail because stakeholders have the wrong mental model. When leaders assume the model is a database, they expect perfect recall and perfect truth. When delivery teams assume the model will infer intent, they under-specify tasks and get inconsistent outputs. When security teams assume prompts enforce policy, they over-trust text instructions and under-invest in enforcement.

LLM fundamentals matter because they shape every major decision in the engagement: what use cases are feasible, what risks are unacceptable, what system components are required (RAG, tools, guardrails), and what the quality bar must be. If the team cannot explain how tokens, context windows, and non-determinism work, they cannot defend cost estimates, latency targets, or governance gates.

## 1.1. The Consulting Decisions This Enables

Understanding LLM fundamentals enables decisions like:

- Should this be an LLM solution at all, or a deterministic system (search/SQL/rules)?
- Should we use RAG, tool calling, or both?
- What is an acceptable error budget, and where do we need human-in-the-loop?
- What does "good enough" mean for POC vs pilot vs production?
- What model class fits the budget and latency targets?
- What data can legally and safely be sent to a model provider?

## 1.2. The Enterprise Constraints That Dominate Outcomes

In enterprise, the constraints that dominate are usually:

- **Quality:** correctness, completeness, groundedness, and instruction adherence
- **Safety:** policy compliance, data leakage prevention, authorization boundaries
- **Cost:** tokens per task times volume (cost per successful task matters most)
- **Latency:** user experience and workflow fit (p95 matters, not only average)

LLM fundamentals are the reason these constraints exist. The model is not "slow" by accident; it generates sequentially. It is not "sometimes wrong" by accident; it optimizes plausibility. It is not "expensive" by accident; cost scales with processed tokens.

---

# 2. WHAT AN LLM IS (AND WHAT IT IS NOT)

**An LLM is a model trained to predict the next token given the previous tokens.** In deployment, you give it text (the prompt), and it produces text (the completion). Everything else we do in enterprise systems (retrieval, tools, guardrails, evals) is about making that behavior safer, more reliable, and more useful.

## 2.1. What It Is: Next-Token Prediction Under Constraints

At inference time, an LLM performs a repeated loop:

1) read the input tokens (prompt + context)
2) compute probabilities for the next token
3) sample or choose the next token (decoding)
4) append it to the context
5) repeat until stopping conditions are met

This is why **generation is sequential** and why long outputs increase latency. It is also why "thinking" is not guaranteed: the model can produce confident-looking text even when evidence is missing.

## 2.2. What It Is Not: A Database, Search Engine, or Truth Machine

It helps to state what an LLM is not, because these are the most common expectation traps:

- Not a database: it cannot "look up" facts unless you provide them (RAG) or call tools
- Not deterministic by default: the same prompt can yield different outputs
- Not internally verifying truth: it does not check sources unless you build that behavior into the system
- Not real-time: its trained knowledge is bounded by training data and does not update automatically

**This does not make LLMs useless; it makes them predictable. Enterprise success comes from building around these properties.**

## 2.3. Training vs Inference (Why This Matters Practically)

Consulting teams do not need training math, but they do need one key distinction:

- Training builds the model's general patterns (its "parametric knowledge").
- Inference is where you apply it to a specific task with a prompt.

**Most enterprise projects should not start with fine-tuning. They should start with good prompts, grounding via RAG, and safe tool access.** Fine-tuning can help with style, consistency, or domain patterns, but it does not replace the need for authoritative sources and evaluation.

---

# 3. HOW LLMs WORK IN PRACTICE (TOKENS, CONTEXT, AND DECODING)

This section covers the mechanics you need to estimate feasibility, cost, latency, and reliability.

## 3.1. Tokens: The Unit That Controls Cost and Latency

**LLMs do not operate on words; they operate on tokens (text fragments).** In practice, tokens are the unit that matters because:

- providers price by tokens (input + output)
- generation happens token-by-token (latency grows with output length)
- the context window is measured in tokens

Rules of thumb that help in discovery and scoping:

- 1 dense A4 page is often ~500-700 tokens
- a 10-turn chat can be ~2k-5k tokens depending on verbosity
- code, tables, and JSON are token-heavy (symbols and structure add tokens)

These are approximations, but they are good enough to identify obviously infeasible ideas (for example, "put all PDFs into context").

## 3.2. Context Window: The Model's Working Memory

**The context window is the maximum number of tokens the model can "see" at once (prompt + conversation history + retrieved context + tool outputs + the response being generated).** If the prompt exceeds the window, something will be truncated or dropped, and quality will degrade in ways that are hard to predict.

Enterprise implications:

- Long documents require retrieval or summarization; you cannot rely on "just paste it"
- Long conversations require memory strategies (summaries, sliding windows, state)
- Large contexts increase cost and can reduce quality by adding noise

This is where RAG becomes an architectural necessity, not a nice-to-have.

## 3.3. Decoding Parameters: Determinism vs Exploration

When the model chooses the next token, you control how it chooses:

- Temperature: higher means more variation; lower means more deterministic outputs
- Top-p / top-k: how much of the probability mass is considered for sampling
- Max tokens: caps output length and cost

In enterprise systems, you usually want outputs that are stable enough to test. That means low temperature for most tasks, strict output contracts where possible, and evals to measure whether your settings produce acceptable behavior.

## 3.4. Why Models Can Sound Confident When They Are Wrong

Confidence in LLM text is not evidence of correctness. **The model is optimized to produce fluent continuations. When it lacks evidence, it may still produce a plausible answer because that is the statistically likely continuation in its learned space. This is why "trust" is a system property: you need grounding, abstention behavior, and measurement.**

For practical mitigation, see:
- [`1.3_hallucinations_basics.md`](./1.3_hallucinations_basics.md) for failure modes and mitigations
- [`../02_solution_components/2.5_evals.md`](../02_solution_components/2.5_evals.md) for measurement and gating

---

# 4. ENTERPRISE TRADE-OFFS (QUALITY, COST, LATENCY, AND SAFETY)

LLM programs live inside constraints. This section turns mechanics into consulting trade-offs you can explain and defend.

## 4.1. Quality vs Cost (Why “Best Model” Is Not a Strategy)

Stronger models can improve quality, but they can also increase cost. The right metric is rarely "cost per request"; it is **cost per successful task**. A cheaper model that fails more often may cost more after re-asks, escalation, and longer prompts.

A consulting-friendly way to estimate:

- Estimate typical input tokens and output tokens per task.
- Multiply by expected monthly volume.
- Compare across model options using a realistic success rate assumption.

In early stages, it is often better to pick a cost-efficient baseline and use evals to justify upgrades only when needed.

## 4.2. Latency vs Thoroughness (The “Fast Enough” Question)

**Longer prompts, more retrieval, reranking, and more guardrails can improve safety and quality but add latency.** The key metric is p95 latency: it reflects user experience on the worst common cases. If p95 is too high, adoption suffers even if average is acceptable.

Consulting guidance:
- Keep UX targets explicit (for example, p95 under 5 seconds)
- Use streaming where appropriate, but do not hide long delays behind spinners
- Measure latency by component (retrieval vs tools vs model) so you know what to optimize

## 4.3. Safety and Data Handling (The Real Enterprise Risk)

Enterprise risk is not only "bad answers". It includes data leakage, unauthorized actions, and policy bypass. Prompts can describe boundaries, but they cannot enforce them. Safety requires:

- access control (who can see what, who can do what)
- data minimization (send only what is needed)
- redaction (remove PII/secrets before sending or logging)
- tool allowlists and argument validation (server-side)
- monitoring and incident response (treat safety failures as incidents)

For enforcement patterns, see [`../02_solution_components/2.4_guardrails.md`](../02_solution_components/2.4_guardrails.md).

## 4.4. Reliability and Change Risk (Why You Need Gates)

LLM behavior can change because:
- prompts change (intentional)
- models update (provider behavior)
- documents change (RAG corpus drift)
- tools change (API behavior)
- user behavior changes (new intents, new ambiguity)

This is why enterprises need evals and monitoring. Without gates, teams will ship changes that regress quality or safety without noticing until trust is damaged.

---

# 5. HOW TO DESIGN ENTERPRISE SOLUTIONS AROUND LLM LIMITATIONS

LLM fundamentals translate into architectural patterns. This section explains when to choose each pattern and what to watch for.

## 5.1. When Plain Prompting Is Enough

Plain prompting works best when:

- the task is mostly transformation or drafting (summaries, classification, rewriting)
- the stakes are moderate and outputs are reviewed
- the content is already in the prompt and the context is small

Even then, you should prefer structured outputs if downstream systems consume results, and you should create a small eval set if the prompt becomes important.

## 5.2. When You Need RAG (Grounding and Citations)

Use RAG when:

- answers must be traceable to enterprise sources (policy, procedures, product docs)
- content changes frequently (knowledge must be up to date)
- different users have different access (ABAC at retrieval time)

RAG is not just a technical feature; it is a trust mechanism. When users can see citations, they can verify and adopt faster.

See [`../02_solution_components/2.1_rag.md`](../02_solution_components/2.1_rag.md).

## 5.3. When You Need Tool Calling (Real-Time State and Actions)

Use tools when:

- the question requires real-time state (inventory, ticket status, account info)
- the system must take actions (create a ticket, update a record, trigger a workflow)

The key separation is **selection vs execution**. The model may propose tool usage, but the system must enforce authorization, validate arguments, and confirm outcomes based on tool outputs.

See [`../02_solution_components/2.2_tool_calling.md`](../02_solution_components/2.2_tool_calling.md).

## 5.4. Why Guardrails Are Not Optional in Enterprise

Guardrails are the enforcement layer. They prevent harmful outputs, unauthorized actions, and data exposure. In enterprise, guardrails convert policies into technical controls.

See [`../02_solution_components/2.4_guardrails.md`](../02_solution_components/2.4_guardrails.md).

## 5.5. The Minimal “Enterprise-Ready” Stack

A pragmatic baseline stack for many enterprise assistants:

- prompt contract (role/task/constraints/output format)
- grounding (RAG with citations) for factual knowledge tasks
- tools (allowlisted) for real-time state and actions
- guardrails (policy + validation + redaction)
- evals (offline) + monitoring (online) to manage change and drift

This is the shortest path to a system that can scale beyond a demo.

---

# 6. HOW TO MEASURE AND OPERATE LLM SYSTEMS 

LLM fundamentals become a delivery capability when teams operationalize measurement and change control.

Treat prompt, retrieval, and tool changes as product changes. They must be measured. A minimal discipline:

- define rubrics with explicit P0 blockers
- build a v0 eval set (50-100 real cases)
- report results by slice (high-risk, tool cases, hard cases)
- ship controlled changes and compare deltas

This converts "it feels better" into "we can defend this decision".

See [`../02_solution_components/2.5_evals.md`](../02_solution_components/2.5_evals.md).

---

# NEXT STEPS

- [`1.2_prompt_engineering.md`](./1.2_prompt_engineering.md) - How to write prompts as contracts and operate them with gates
- [`1.3_hallucinations_basics.md`](./1.3_hallucinations_basics.md) - Failure modes and layered mitigations for confident wrong answers
- [`../02_solution_components/2.1_rag.md`](../02_solution_components/2.1_rag.md) - Grounding patterns and citations for enterprise knowledge
- [`../02_solution_components/2.2_tool_calling.md`](../02_solution_components/2.2_tool_calling.md) - Safe tool usage, authorization, and reliability patterns
- [`../02_solution_components/2.4_guardrails.md`](../02_solution_components/2.4_guardrails.md) - Enforcement patterns for safety and data handling
- [`../02_solution_components/2.5_evals.md`](../02_solution_components/2.5_evals.md) - Measurement, gates, and operating model for quality and safety
