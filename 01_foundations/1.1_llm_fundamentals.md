# LLM FUNDAMENTALS

**Goal:** Build a correct mental model of LLMs for technical and strategic decision-making in a corporate context.
**Prerequisites:** None (this is the starting point).
**Related:**
- [`prompt_engineering.md`](./prompt_engineering.md) — How to instruct LLMs effectively
- [`hallucinations_basics.md`](./hallucinations_basics.md) — Understanding and mitigating hallucinations
- [`../02-solution-components/rag.md`](../02-solution-components/rag.md) — Expanding model knowledge with corporate data
- [`../06-risk-governance-ethics/data_privacy_and_security.md`](../06-risk-governance-ethics/data_privacy_and_security.md) — Security implications

---

## What's In / What's Out

**In:**
- Operational mental model of how LLMs work (tokens, context, prediction)
- Fundamental limitations and their practical implications in business
- Core trade-offs (cost vs. quality, latency vs. context, temperature vs. determinism)
- Common expectation antipatterns in corporate projects

**Out:**
- Training details (backpropagation, transformer architecture, RLHF)
- Technical prompt implementation (moved to `prompt_engineering.md`)
- Specific hallucination mitigations (moved to `hallucinations_basics.md`)
- Solution architectures (moved to the `02-solution-components/` folder)

---

## 1. EXECUTIVE SUMMARY:

1. **Statistical Prediction Engine**: An LLM predicts "which word comes next" based on learned patterns from trillions of texts. It is not a searchable database nor does it have "true knowledge."

2. **Works with Tokens, Not Words**: Text is fragmented into pieces (tokens ≈ 0.75 words). Cost, speed, and technical limits are measured in tokens.

3. **Limited Context = Limited Memory**: Can process only N tokens at a time (4k–200k). Long conversations or large documents require strategies (RAG, summaries, chunking).

4. **Non-deterministic and Prone to Hallucinations**: Can invent believable but false information. Requires controls (evals, citations, human-in-the-loop) for corporate use.

5. **Cost + Latency + Security are Non-trivial**: Each request costs money (processed tokens), takes time (sequential generation), and exposes data to the provider (residency, retention, DLP).

---

## 2. OPERATIONAL DEFINITION

- What LLM IS
    - An AI system trained on massive amounts of text to predict likely sequences of tokens.
    - Capable of generating coherent text, answering questions, summarizing, translating, writing code, and classifying content.
    - Operated via API (prompt → response) with control through hyperparameters (temperature, top-p, max tokens).

- What LLM IS NOT
    - Not a database: does not "store facts" to be queried; reconstructs statistical patterns.
    - Not deterministic: the same input can generate different outputs (unless temperature = 0 and seed is fixed).
    - Not "intelligent" in the human sense: does not reason logically or internally verify truth.
    - Does not have access to real-time data: knowledge is frozen at the training data cutoff date.

---

## 3. TOKENS, CONTEXT, AND PREDICTION

### 3.1. Tokens (Basic Processing Unit)

**Practical definition**: Tokens are text fragments (~0.75 words in Portuguese/English).

**Examples**:
- "Olá" = 2 tokens: `["Ol", "á"]`
- "strategy" = 1-2 tokens
- Source code and JSON consume more tokens (symbols = individual tokens).

**Why it matters for businesses**:
- **Cost**: Providers charge per token (input + output). E.g., GPT-4: $0.03/1k input tokens, $0.06/1k output tokens.
- **Latency**: Generation is sequential (1 token at a time). A 500-token response = 500 "steps."
- **Context**: Limited window (e.g., 128k tokens = ~96k words). Large documents may exceed this limit.

**Rule of thumb for estimation**:
- 1 dense A4 page ≈ 500–700 tokens
- 1 hour of transcribed meeting ≈ 5k–10k tokens
- 1 typical chatbot conversation (10 turns) ≈ 2k–5k tokens

### 3.2. Context Window

**Definition**: The maximum number of tokens the model "sees" at once (prompt + history + response).

**Common models**:
- GPT-4o: 128k tokens
- GPT-5.2: 256k tokens
- Claude 4.5 Sonnet: 200k tokens
- Gemini 3 Pro: 2M tokens

**Practical implications**:
- **Long documents**: Must be fragmented (chunking) or summarized before use.
- **Long conversations**: History grows indefinitely → summary strategy or mandatory sliding window.
- **Increasing cost**: Larger context = more processed tokens = more expensive per request.

**Common antipattern**: "Let's put all company PDFs into the context" → unfeasible (technical limits + cost + noise).

### 3.3. Next-Token Prediction (Core Mechanism)

An LLM doesn't "think" or "understand"; it **statistically predicts** which token has the highest probability of appearing next.

**Visual example**:
```
Prompt: "The capital of Brazil is"
LLM calculates probabilities:
- "Brasilia" → 92%
- "São" → 3%
- "Rio" → 2%
- ...
```

**Temperature (critical parameter)**:
- `temperature = 0`: Always chooses the most probable token (deterministic, repetitive).
- `temperature = 0.7`: Introduces variation (creative, useful for writing).
- `temperature = 1.5`: Very random (incoherent, not recommended).

**When to use each temperature**:
- **T=0**: Data extraction, classification, structured analysis (must be repeatable).
- **T=0.3-0.7**: Creative generation, brainstorming, content writing.
- **T>1.0**: Rarely useful (except for experimentation).

---

## 4. PRACTICAL IMPLICATIONS

### 4.1. Auditability and Traceability

**Problem**: LLMs generate responses without "showing their work." In auditing or compliance, there is no reasoning log.

**Implications**:
- **High-risk cases** (legal, financial, health): Require **explicit citations** of sources, human validation, or abstention.
- **Mandatory logging**: Complete request (prompt + context + parameters + response) for future investigation.
- **Versioning**: Model or prompt changes can alter behavior → formal change control.

**Practical solution**: Architecture with RAG + citations (link: [`../02-solution-components/rag.md`](../02-solution-components/rag.md))

### 4.2. Predictable Cost and FinOps

**Problem**: Cost is a function of the volume of processed tokens. Without control, costs can explode.

**Cost factors**:
- Prompt size (context injected in each request).
- Response size (limited via `max_tokens`).
- Frequency of calls.
- Model choice (GPT-5.2 vs. GPT-4o = 3–5x difference).

**Control strategies**:
- **Context caching**: Avoid reprocessing the same context repeatedly.
- **Intelligent routing**: Use a smaller model (GPT-4o) for simple queries, larger model (GPT-5.2) only when necessary.
- **Quotas per user/team**: Token limit per day to prevent abuse.
- **Continuous monitoring**: Cost dashboard per use case, alerts if > threshold.

**Rule of thumb**: MVP with 100 active users, 20 queries/day/user, 1k tokens of context, 500-token response:
- 100 × 20 × 1.5k tokens/day = 3M tokens/day
- GPT-5.2: ~$45–60/day → $1.35k–1.8k/month

**Link**: [`../02-solution-components/observability_llmops.md`](../02-solution-components/observability_llmops.md) for FinOps strategies.

### 4.3. Latency and User Experience

**Problem**: Generation is sequential (token streaming). A long response = noticeable wait time.

**Typical benchmarks**:
- GPT-4: 20–40 tokens/sec
- Claude 3.5 Sonnet: 40–80 tokens/sec
- 500-token response → 8–25 seconds

**UX implications**:
- **Mandatory streaming**: Show tokens as they arrive (better perception of progress).
- **Realistic timeout**: Complex responses can take 30–60s → do not use a 10s timeout.
- **Experience progression**: Start with a loading state, then streaming, then the final response.

**Antipattern**: Hiding latency with a generic spinner → user thinks the system has frozen.

### 4.4. Data Security and Privacy

**Problem**: The prompt contains sensitive corporate data. The LLM provider may retain/use data to train future models.

**Risks**:
- **PII/secrets leakage**: SSN, passwords, contracts in the prompt.
- **Data residency**: Request processed in a data center outside the country (LGPD/GDPR).
- **Indefinite retention**: Provider keeps logs for their own compliance.

**Mandatory controls**:
- **Enterprise contracts with zero retention**: Contractual guarantee of no retention and no training.
- **DLP (Data Loss Prevention)**: Automatic redaction of PII/secrets before sending to the LLM.
- **On-premise or VPC**: Self-hosted models (e.g., Llama, Mistral) if data is hypersensitive.
- **Data classification**: Only "public" or "low-risk internal" data in public API LLMs.

**Canonical link**: [`../06-risk-governance-ethics/data_privacy_and_security.md`](../06-risk-governance-ethics/data_privacy_and_security.md)

### 4.5. Reliability and Hallucinations

**Problem**: LLMs can generate false information with apparent confidence (hallucination).

**Critical scenarios**:
- Responding with the wrong procedure (operations, security).
- Citing non-existent regulations (legal, compliance).
- Inventing metrics or data (finance, BI).

**Layered mitigation principle**:
1. **Explicit abstention instruction**: "If you don't know, say 'I don't know'" --> this strategy is already naturally implemented in more recent models.
2. **RAG with citations**: Response must come from a traceable source.
3. **Human validation**: HITL (human-in-the-loop) for critical decisions.
4. **Continuous Evals**: Detect quality regression when changing models/prompts.

**Link**: [`hallucinations_basics.md`](./hallucinations_basics.md) for details.

---

## 5. DECISION CHECKLIST: WHEN TO RECOMMEND LLM USE?

**Positive Signs (LLM makes sense)**
[ ] Task involves natural language (text, code, structuring).
[ ] No simple deterministic rule exists (e.g., it's not just regex or SQL).
[ ] Controlled margin of error is tolerated (with human validation or guardrails).
[ ] Data volume justifies cost (not a daily heavy batch of millions of records).
[ ] Input data is not hypersensitive OR there is a DLP/redaction strategy.

**Warning Signs (caution required)**
[ ] Critical high-impact decision (financial, legal, security) → requires HITL.
[ ] Zero tolerance for error → maybe not a case for LLM alone.
[ ] Highly sensitive data (PII, secrets) → requires on-premise model or redaction.
[ ] Critical latency (< 1 second) → may be unfeasible depending on the case.
[ ] Extreme volume (millions of requests/day) → cost may be prohibitive.

**Blockers (LLM is not the right tool)**
[ ] Task has a simple and cheap deterministic solution (e.g., SSN validation).
[ ] Requires perfect mathematical precision (complex financial calculations).
[ ] LLM training data does not cover the specific domain AND there is no viable RAG.
[ ] Compliance explicitly prohibits the use of generative AI (regulated sectors).

---

## 6. DISCOVERY QUESTIONS (FOR USE WITH CLIENTS)

### 6.1. About expectations and use cases
1. **"What specific decision or task should the LLM support?"** → Goal: Avoid vague "we want GenAI"; force specificity.
2. **"What margin of error is acceptable? Zero? 5%? 20%?"** → Goal: Identify high-risk cases that require HITL (Human in the Loop).
3. **"Is there a source of truth to validate LLM answers?"** → Goal: Verify feasibility of RAG + citations.

### 6.2. About data and context
4. **"How many documents/records need to be consulted? What is the total size?"** → Goal: Estimate context volume and technical feasibility of RAG.
5. **"Does data contain PII, secrets, or sensitive information?"** → Goal: Identify need for DLP/redaction or an on-premise model.
6. **"How often does data change? Daily? Monthly?"** → Goal: Understand the need for corpus updates.

### 6.3. About volume and cost
7. **"How many users? How many queries per day are expected?"** → Goal: Estimate monthly cost and financial feasibility.
8. **"What latency is acceptable? 5 seconds? 30 seconds?"** → Goal: Validate if the LLM meets UX requirements.

### 6.4. About governance and risk
9. **"Who approves the deployment of a prompt or model change?"** → Goal: Establish change control from the start.
10. **"How will it be audited in case of error or complaint?"** → Goal: Ensure adequate logging and traceability.

---

## NEXT STEPS

1. **To better understand how to use LLMs**: read [`prompt_engineering.md`](./prompt_engineering.md)
2. **To mitigate hallucination risks**: read [`hallucinations_basics.md`](./hallucinations_basics.md)
3. **To understand a solution architecture**: start with [`../02-solution-components/rag.md`](../02-solution-components/rag.md)