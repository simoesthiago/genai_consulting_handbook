# HALLUCINATION BASCIS

**Objective:** Understand, identify, and mitigate LLM hallucinations in enterprise contexts where accuracy and trust are critical.

**When to use:**
- When designing high-stakes use cases (legal, financial, healthcare, compliance)
- To establish quality gates and acceptance criteria for LLM outputs
- When debugging accuracy issues or loss of trust
- To explain hallucination risk to non-technical stakeholders (executives, legal, compliance)

**Prerequisites:**
- [`llm_fundamentals.md`](./llm_fundamentals.md) — Understanding how LLMs work (prediction vs retrieval)
- [`prompt_engineering.md`](./prompt_engineering.md) — How to instruct LLMs effectively

**Related:**
- [`../02-solution-components/rag.md`](../02-solution-components/rag.md) — Grounding LLM responses in verifiable sources
- [`../02-solution-components/guardrails.md`](../02-solution-components/guardrails.md) — Technical controls for safety
- [`../02-solution-components/evals.md`](../02-solution-components/evals.md) — Testing and measuring hallucination rates
- [`../06-risk-governance-ethics/quality_and_evals_governance.md`](../06-risk-governance-ethics/quality_and_evals_governance.md) — Governance frameworks

---

## What's In / What's Out

**In:**
- Operational definition of hallucination for enterprise contexts
- Common causes and scenarios where hallucinations occur
- Layered mitigation strategies (prompt design, RAG, validation, HITL)
- How to measure and monitor hallucination rates
- How to explain hallucination risk to C-level and compliance teams

**Out:**
- Deep technical details of LLM architecture (attention mechanisms, training dynamics)
- Advanced research techniques (fine-tuning, RLHF optimization)
- Tool-specific implementation details (goes to solution component docs)
- Domain-specific hallucination patterns (e.g., medical, legal — handle in use case archetypes)

---

## 1. EXECUTIVE SUMMARY

1. **Hallucinations are not bugs, they're features**: LLMs are trained to predict plausible text, not retrieve facts. They will confidently generate false information when uncertain because that's how they're designed.

2. **Risk scales with stakes**: A hallucinated restaurant recommendation is annoying. A hallucinated legal precedent or financial regulation can cause legal liability, compliance violations, or financial loss.

3. **Trust erosion is rapid and hard to recover**: Users who encounter one confident hallucination will distrust all future outputs, even correct ones. Trust is earned slowly but lost instantly.

4. **Mitigation requires layers**: No single technique eliminates hallucinations. Production systems need: explicit abstention prompts + RAG with citations + output validation + human-in-the-loop for high-risk decisions.

5. **Measurability is mandatory**: You can't improve what you don't measure. Hallucination rate must be tracked with golden sets, user feedback, and spot audits (see [`../02-solution-components/evals.md`](../02-solution-components/evals.md)).

---

## 2. OPERATION DEFINITION: WHAT IS HALLUCINATION?

### 2.1. Definition for Enterprise

**Hallucination:** LLM output that is factually incorrect, unverifiable, or inconsistent with provided context, yet presented with confidence as if it were true.

**Key characteristics:**
- **Factually incorrect:** Contradicts known facts (e.g., wrong date, wrong person, wrong number)
- **Unverifiable:** Cannot be traced to a source (e.g., invented citation, fabricated statistic)
- **Inconsistent with context:** Contradicts information in the prompt or retrieved documents
- **Presented with confidence:** No indication of uncertainty (no "I'm not sure", no hedging)

---

### 2.2. What Hallucination Is NOT

- **Not a misunderstanding or ambiguity:** If the input is unclear and LLM makes a reasonable interpretation, that's not hallucination.
- **Not a difference of opinion:** Subjective judgments (e.g., "This movie is good") are not hallucinations.
- **Not outdated knowledge:** If LLM's knowledge cutoff is Jan 2025 and it doesn't know about Feb 2025 events, that's expected limitation, not hallucination (unless it invents an answer instead of saying "I don't know").
- **Not a simple mistake:** If LLM miscalculates 127 + 456 = 582 (correct: 583), that's a calculation error, not hallucination (though mitigation strategies overlap).

---

### 2.3. Types of Hallucination (Taxonomy)

**Type 1: Factoid Hallucination**
- **Definition:** Inventing specific facts (dates, names, numbers, events)
- **Example:** "The Eiffel Tower was completed in 1892" (Correct: 1889)
- **Risk level:** High (easily fact-checked, damages credibility immediately)

**Type 2: Source Hallucination**
- **Definition:** Inventing citations, references, or attributions
- **Example:** "According to Smith et al. (2023), ..." (paper doesn't exist)
- **Risk level:** Very High (academic/legal contexts rely on verifiable sources)

**Type 3: Reasoning Hallucination**
- **Definition:** Fabricating logical steps or causal relationships not supported by evidence
- **Example:** "Since revenue increased 5%, we can conclude market share grew" (non-sequitur)
- **Risk level:** Medium-High (harder to detect, can lead to bad decisions)

**Type 4: Context Hallucination**
- **Definition:** Contradicting information in the prompt or retrieved documents
- **Example:** Document says "Q4 revenue: $10M", LLM says "Q4 revenue: $12M"
- **Risk level:** Very High (demonstrates LLM is not using provided context correctly)

**Type 5: Procedural Hallucination**
- **Definition:** Inventing processes, policies, or regulations
- **Example:** "According to LGPD Article 45..." (article doesn't exist or doesn't say that)
- **Risk level:** Critical (compliance violations, legal liability)

---

## 3. COMMON CAUSES OF HALLUCINATION

### 3.1. Uncertainty in Training Data

**Mechanism:** LLM encounters question it wasn't trained on or has conflicting training examples.

**Example:**
- Prompt: "What is the population of [obscure town]?"
- LLM: *Has no data, but trained to always answer* → Generates plausible-sounding number

**Why it happens:** LLMs are trained to minimize loss (predict next token), not to admit uncertainty.

---

### 3.2. Prompt Doesn't Include Abstention Instructions

**Mechanism:** LLM defaults to generating an answer even when it should refuse.

**Bad prompt:**
```
What is the capital of Wadiya?
```

**LLM response:** "The capital of Wadiya is Wadiya City." (Wadiya is fictional)

**Good prompt:**
```
What is the capital of Wadiya? If you don't know or if Wadiya doesn't exist, respond with "I don't have information about this."
```

**LLM response:** "I don't have information about Wadiya as a real country."

---

### 3.3. Context Window Limits (Lost in the Middle)

**Mechanism:** LLM loses track of information in the middle of a long context.

**Example:**
- Context: 50k tokens of documents
- Relevant info is at token 25k (middle)
- LLM focuses on beginning and end → hallucinates info from middle

**Mitigation:** RAG with chunking and reranking (see [`../02-solution-components/rag.md`](../02-solution-components/rag.md)).

---

### 3.3. Ambiguous or Leading Prompts

**Mechanism:** Prompt assumes a fact that isn't true, LLM goes along with it.

**Example:**
```
What year did Steve Jobs become CEO of Microsoft?
```

**LLM (hallucinating):** "Steve Jobs never became CEO of Microsoft, but he co-founded Apple in 1976..." (Correct refusal)

**vs.**

**LLM (vulnerable to leading question):** "Steve Jobs became CEO of Microsoft in 1997." (Hallucination — he never was)

**Best practice:** Avoid presuppositions. Rephrase as: "Was Steve Jobs ever CEO of Microsoft? If yes, when?"

---

### 3.4. Insufficient Grounding (No RAG)

**Mechanism:** LLM relies solely on parametric knowledge (training data) instead of retrieving from authoritative sources.

**Example:**
- Task: Answer questions about company policies
- No RAG: LLM invents policies based on "typical" policies
- With RAG: LLM retrieves exact policy from company knowledge base

**Critical insight:** Parametric knowledge is always approximate. For enterprise accuracy, RAG is mandatory.

---

## 4. LAYERED MITIGATION STRATEGY

### 4.1. Prompt Design (First Line of Defense)

**Technique 1.1: Explicit Abstention Instructions**

**Pattern:**
```
[TASK DESCRIPTION]

CRITICAL: If you don't have sufficient information to answer accurately, respond with:
"I don't have enough information to answer this question accurately."

Do NOT guess, speculate, or generate plausible-sounding answers.
```

**Example:**
```
You are a customer service assistant for a bank.

Answer the user's question based ONLY on the information in the knowledge base below.

CRITICAL: If the answer is not in the knowledge base, respond exactly with:
"I don't have information about this in our current knowledge base. Please contact customer service at 1-800-XXX-XXXX."

Knowledge base:
[RETRIEVED CHUNKS]

User question: [USER INPUT]
```

---

**Technique 1.2: Force Source Attribution**

**Pattern:**
```
When answering, ALWAYS cite the specific source document and section where you found the information.

Format: "According to [Document Name, Section X], ..."

If you cannot find a source, say "I cannot find information about this in the provided documents."
```

**Example output:**
```json
{
  "answer": "The company's return policy allows returns within 30 days of purchase.",
  "source": "Return Policy Document v2.3, Section 2.1",
  "confidence": "high",
  "quote": "Customers may return items within 30 days of purchase for a full refund."
}
```

---

**Technique 1.3: Request Confidence Scores**

**Pattern:**
```
For each answer, provide a confidence level:
- high: Information is explicitly stated in the source
- medium: Information is implied or requires minor inference
- low: Information is uncertain or partially available

If confidence is "low", default to abstention.
```

---

### 4.2. RAG with Citations (Ground Truth)

**Why RAG reduces hallucination:**
- LLM retrieves from authoritative sources instead of relying on parametric memory
- Citations make outputs verifiable (user can check source)
- Retrieval failure is detectable (no chunks returned = abstain)

**Critical RAG practices for hallucination prevention:**

1. **Chunk attribution:** Each retrieved chunk must have metadata (doc_id, section, page)
2. **Citation enforcement:** Prompt must require citations for every claim
3. **Retrieval quality monitoring:** Track retrieval precision/recall (see [`../02-solution-components/rag.md`](../02-solution-components/rag.md))
4. **Fallback to abstention:** If retrieval confidence < threshold, abstain instead of answering

**Example RAG prompt with anti-hallucination controls:**
```
You are answering questions using the retrieved documents below.

RULES:
1. Answer ONLY using information from the retrieved documents
2. ALWAYS cite the document ID and section for each claim
3. If the retrieved documents don't contain the answer, respond: "I cannot answer this based on the available documents."
4. Do NOT use your general knowledge to fill gaps

Retrieved documents:
[CHUNKS WITH METADATA]

Question: [USER QUESTION]

Response format:
{
  "answer": "string (your answer with inline citations)",
  "sources": [
    {"doc_id": "string", "section": "string", "quote": "string"}
  ],
  "confidence": "high|medium|low"
}
```

---

### 4.3. Output Validation (Catch Escapes)

**Validation checks to catch hallucinations:**

**Check 1: Source Verification**
- Does every claim have a citation?
- Do cited sources actually exist in the knowledge base?
- Does the quote match the source document?

**Check 2: Consistency Validation**
- Does the answer contradict the retrieved context?
- Are there internal contradictions in the answer?

**Check 3: Fact-Checking (Domain-Specific)**
- Does the answer violate known constraints? (e.g., negative revenue, date in the future)
- Does it pass basic sanity checks? (e.g., percentages sum to 100%, dates are chronological)

**Example validation code:**
```python
def validate_answer(answer: dict, retrieved_chunks: list) -> bool:
    # Check 1: All sources exist
    cited_doc_ids = [s["doc_id"] for s in answer.get("sources", [])]
    available_doc_ids = [chunk["doc_id"] for chunk in retrieved_chunks]
    
    if not all(doc_id in available_doc_ids for doc_id in cited_doc_ids):
        logger.warning("Answer cites non-existent documents")
        return False
    
    # Check 2: Quote verification (fuzzy match)
    for source in answer.get("sources", []):
        quote = source.get("quote", "")
        doc_content = next((c["content"] for c in retrieved_chunks if c["doc_id"] == source["doc_id"]), "")
        
        if quote and quote not in doc_content:
            # Allow fuzzy match (Levenshtein distance)
            if fuzz.partial_ratio(quote, doc_content) < 80:
                logger.warning(f"Quote mismatch for doc {source['doc_id']}")
                return False
    
    # Check 3: Confidence threshold
    if answer.get("confidence") == "low":
        logger.info("Low confidence answer, flagging for review")
        return False
    
    return True
```

---

### 4.4. Human-in-the-Loop (Final Safeguard)

**When HITL is mandatory:**
- High-stakes decisions (legal, financial, healthcare)
- First-time use case (no historical accuracy data)
- User-reported hallucination (incident response)
- Confidence score below threshold

**HITL patterns:**

**Pattern 1: Pre-Approval (Blocking)**
```
1. LLM generates response
2. Response is held in queue
3. Human reviews and approves/rejects/edits
4. Only approved responses are sent to user
```

**Use for:** Legal advice, financial transactions, sensitive communications

---

**Pattern 2: Post-Audit (Non-Blocking)**
```
1. LLM generates response
2. Response is sent to user immediately
3. Human audits sample (e.g., 5% random, all low-confidence)
4. Hallucinations trigger prompt revision or retraining
```

**Use for:** Customer service, content generation, internal tools

---

**Pattern 3: Escalation (Conditional)**
```
1. LLM generates response
2. If confidence < threshold OR user disputes answer:
   - Escalate to human agent
   - Human provides correct answer
   - Log as training data for future improvement
```

**Use for:** Hybrid chatbots, technical support

---

## 5. MEASURING HALLUCINATION RATE

**You can't improve what you don't measure.**
- Baseline hallucination rate (before mitigation)
- Track improvement over time (after prompt changes, RAG improvements)
- Detect regression (model updates, prompt drift)
- Set acceptance criteria (e.g., < 2% hallucination rate for production)

### 5.1. Measurement Methods

**Method 1: Golden Set Evaluation (Offline)**

1. **Create golden set:** 50-200 questions with known correct answers
2. **Include hard cases:** Edge cases, trick questions, "I don't know" scenarios
3. **Run LLM on golden set**
4. **Manual or automated scoring:**
   - Correct answer: 1 point
   - Hallucination (factually wrong): 0 points
   - Proper abstention ("I don't know" when correct): 1 point
   - Improper abstention (refuses to answer when it should): 0.5 points

**Hallucination rate = (# hallucinations) / (# questions)**

**Link:** [`../02-solution-components/evals.md`](../02-solution-components/evals.md) for detailed eval setup.

---

**Method 2: User Feedback (Online)**

- Add "Was this answer helpful?" / "Report incorrect answer" button
- Track feedback rate and content
- Sample and audit reported hallucinations
- Calculate: **Hallucination rate = (# reported hallucinations) / (# total answers)**

**Caveat:** Users don't report all hallucinations (detection rate varies by domain expertise).

---

**Method 3: Spot Audits (Hybrid)**

- Randomly sample 5-10% of production outputs
- Human experts manually review for accuracy
- Score each output: correct / hallucination / abstention
- Extrapolate to estimate overall hallucination rate

**Frequency:** Weekly for new systems, monthly for mature systems.

---








## NEXT STEPS

1. **To ground LLM outputs in verifiable sources:** Read [`../02-solution-components/rag.md`](../02-solution-components/rag.md)
2. **To implement technical safety controls:** Read [`../02-solution-components/guardrails.md`](../02-solution-components/guardrails.md)
3. **To measure and test hallucination rates:** Read [`../02-solution-components/evals.md`](../02-solution-components/evals.md)
