# PROMPT ENGINEERING

**Goal:** Help consultants and delivery teams design prompts that are **reliable, testable, and safe** in enterprise contexts, turning "good demos" into systems that can scale with governance, measurement, and operational discipline.

**Prerequisites:**
- [`1.1_llm_fundamentals.md`](./1.1_llm_fundamentals.md) - How LLMs behave (tokens, context limits, non-determinism)
- [`1.3_hallucinations_basics.md`](./1.3_hallucinations_basics.md) - Why "sounds right" is not a quality bar, and what to do about it

**Related:**
- [`../02_solution_components/2.5_evals.md`](../02_solution_components/2.5_evals.md) - Turning prompt changes into measurable decisions and release gates
- [`../02_solution_components/2.4_guardrails.md`](../02_solution_components/2.4_guardrails.md) - Safety controls beyond prompts (policy, filtering, enforcement)
- [`../02_solution_components/2.2_tool_calling.md`](../02_solution_components/2.2_tool_calling.md) - Prompting for tool usage, safety, and action reliability
- [`../02_solution_components/2.1_rag.md`](../02_solution_components/2.1_rag.md) - Prompting for grounding and citations in retrieval-based systems
- [`../03_use_case_archetypes/3.2_genai_copilots_internal.md`](../03_use_case_archetypes/3.2_genai_copilots_internal.md) - How prompting shows up in internal copilots and workflows

---

# TL;DR (30 SECONDS)

Prompt engineering is not "writing nice instructions". In enterprise programs, prompts are a **product specification** for behavior: what the system must do, what it must not do, how it must format outputs, and how it must behave under uncertainty and attack. The only sustainable way to improve prompts is to treat them like versioned assets: define a rubric, run evals, ship controlled changes, and monitor drift in production.

- Prompts are **contracts**, not wishes: role, task, context, constraints, output format
- Use **output contracts** (structured outputs, schemas) whenever downstream automation exists
- Separate **untrusted inputs** (users, retrieved docs) from instructions to reduce injection risk
- Prefer **rubrics + evals** over subjective "looks good" reviews
- Build a prompt **lifecycle**: draft -> evaluate -> gate -> deploy -> monitor -> iterate
- Prompts are necessary but not sufficient: use **guardrails** for enforcement and safety

---

# WHAT'S IN / WHAT'S OUT

**In:** a practical, end-to-end method for writing prompts that work for strategic consulting tasks and production systems: core patterns, how to design prompts as specs, how to test them, and how to operate them under risk and drift.

**Out:** vendor-specific SDK details and deep framework implementations. This guide is intentionally portable across providers and orchestration stacks.

---

# 1. WHY PROMPT ENGINEERING MATTERS IN ENTERPRISE CONSULTING

In enterprise consulting, the gap between a demo and a deployable capability is rarely the model. The gap is **reliability**: the system must behave consistently enough that a client can trust it, operate it, and defend it under scrutiny. Prompt engineering is a large part of closing that gap because prompts are where you specify the intended behavior: the task boundaries, the allowed actions, the quality bar, and the output format.

Prompts matter because they directly affect the four constraints that dominate most enterprise programs: **quality**, **safety**, **cost**, and **latency**. A "better" prompt can reduce hallucinations by enforcing grounding, but it can also increase cost if it adds long context. A "safer" prompt can reduce risk by requiring abstention, but it can also reduce usefulness if it over-refuses. This is why prompt engineering is a decision discipline, not a writing trick.

For consulting teams, prompts are also a coordination artifact. When prompts are treated as informal text, teams argue with screenshots and anecdotes. When prompts are treated as specifications, teams can align on requirements, test changes with evals, and present go/no-go decisions credibly to stakeholders.

## 1.1. What Decisions Prompt Engineering Enables

Prompt engineering should be designed backward from decisions. Typical decisions include:

- Should we ship this capability from POC to pilot?
- Which model/prompt/retrieval configuration is the baseline?
- Should we allow tool actions, and under what constraints?
- Do we need stricter guardrails, human-in-the-loop, or access controls?
- Are cost and latency acceptable for the expected usage volume?

These decisions are not made by "the best prompt". They are made by the best **measured trade-off** for the target context.

## 1.2. When Prompts Are the Root Cause (And When They Are Not)

Many failures that look like "model issues" are actually prompt issues: vague tasks, missing constraints, ambiguous output formats, or missing instructions about uncertainty. However, some failures are not fixable by prompts alone. If retrieval is missing evidence, prompt changes will have diminishing returns. If tools are unsafe, prompts cannot enforce authorization. If policies are required, guardrails must enforce them.

The consulting habit that helps here is to separate concerns:
- **Prompting** controls behavior and format.
- **Retrieval** controls available evidence.
- **Tools** control actions and state changes.
- **Guardrails** enforce boundaries when behavior is risky.
- **Evals and monitoring** tell you whether changes helped or harmed.

---

# 2. WHAT PROMPT ENGINEERING IS (AND IS NOT)

Prompt engineering is the practice of designing the instructions and context that shape model behavior toward a specific outcome. In enterprise settings, it becomes a discipline of specification and control: you define the behavior contract, encode it into prompts and policies, then measure whether it holds across real scenarios.

## 2.1. Prompts as Contracts (Not Requests)

A useful mental model is that a prompt is a contract with these elements:

- **Role:** what expertise and perspective the system should adopt
- **Task:** what it must do (and what "done" means)
- **Context:** what facts, inputs, and evidence it may use
- **Constraints:** what it must not do, and what safety rules apply
- **Output format:** how results must be structured for downstream use

**When these are explicit, prompts become testable. When they are implicit, the system behaves like a probabilistic autocomplete with inconsistent results.**

## 2.2. What Prompt Engineering Is Not

Prompt engineering is not:

- A guarantee of truth (LLMs can still be wrong under uncertainty)
- A substitute for governance, access control, or security enforcement
- A one-time activity (prompts drift as users, data, and models change)
- A single “best prompt” (the right prompt depends on use case, risk, and constraints)

Prompt engineering is a lever. It is powerful, but it must be paired with evals, monitoring, and enforcement.

## 2.3. The Prompt Stack (Where Instructions Come From)

Enterprise systems usually assemble prompts from multiple layers. A common stack looks like:

```
SYSTEM / PLATFORM POLICY  (non-negotiables)
DEVELOPER INSTRUCTIONS    (product behavior contract)
RUNTIME CONTEXT           (retrieved docs, tool outputs, user profile)
USER INPUT                (untrusted)
```

Two principles follow from this stack:

1) **Keep untrusted content separate.** Users and retrieved documents can contain instructions that should not be followed.
2) **Design for modularity.** A stable system prompt plus task-specific prompt modules scales better than one giant prompt.

---

# 3. CORE PROMPT PATTERNS (THE CONSULTING TOOLKIT)

This section gives you reusable patterns that cover most consulting and enterprise assistant use cases. The goal is not to memorize patterns; it is to understand why they work and when they break.

## 3.1. The "RTCCF" Pattern (Role, Task, Context, Constraints, Format)

The most consistently useful pattern is a simple one. You specify:

- Role (who the model is)
- Task (what to do)
- Context (what to use)
- Constraints (what not to do, rules)
- Format (what the output must look like)

This pattern is strong because it turns a vague request into a specification, which is the first step toward repeatable quality.

### 3.1.1. Example (Consulting Analysis Output)

Bad prompt:
"Analyze this situation and tell me what to do."

Better prompt (RTCCF):

```text
ROLE: You are a strategy consultant.

TASK: Create a 1-page decision memo recommending whether to launch the pilot.

CONTEXT:
- Use case: Internal policy assistant (RAG + citations)
- Constraints: p95 latency < 5s, cost per successful task < $0.05
- Risk: Must have zero P0 safety failures in the golden set
- Findings: <insert metrics and key failures>

CONSTRAINTS:
- Do not invent metrics not present in the context.
- If information is missing, state what is missing and how to get it.

FORMAT:
- Sections: Decision, Rationale, Risks, Mitigations, Next 2 weeks plan
```

Notice the consultant-friendly properties: **explicit decision, explicit constraints, and explicit output structure.**

## 3.2. Output Contracts (Structured Outputs and Schemas)

If a prompt is used for automation, a free-form answer is usually the wrong interface. **In production, the prompt should define an output contract**: the shape and constraints of the output. Contracts enable validation, testing, and safe tool usage.

Contracts can be light (a fixed template) or strict (JSON schema). The stricter the downstream integration, the stricter the contract should be.

### 3.2.1. When to Use Strict JSON

Use strict JSON outputs when:

- The output feeds another system (ticket creation, CRM update, routing)
- You need deterministic parsing (no brittle regex parsing)
- You need safety validation (reject missing fields, block risky values)

Use a structured template (not necessarily JSON) when the output is for humans (memos, slide outlines), but still needs consistency.

### 3.2.2. Pattern: "Return ONLY Valid JSON"

```text
OUTPUT FORMAT:
Return ONLY valid JSON with this schema:
{
  "decision": "go|no-go|iterate",
  "rationale": ["..."],
  "risks": [{"risk":"...", "severity":"P0|P1|P2", "mitigation":"..."}],
  "next_steps": ["..."]
}
```

This pattern should be paired with validation in code. Prompts request compliance; guardrails enforce it.

## 3.3. Few-Shot Examples and Rubrics

**Few-shot examples show the model what "good" looks like by demonstration.** Rubrics show the model (and evaluators) how outputs will be judged. In consulting workflows, rubrics are especially useful because they turn subjective tasks (summaries, prioritization) into testable expectations.

Use few-shot examples when:
- The format is unfamiliar or easy to get wrong
- The task has subtle constraints (tone, compliance language, citations)
- You want consistency across a team

Use rubrics when:
- You need to measure improvements (link to evals)
- The task is subjective but still needs repeatability (pass/fail criteria)

## 3.4. Decomposition and Clarifying Questions

Many failures are caused by ambiguous inputs. **A reliable assistant must know when to ask questions instead of guessing.** Prompting should specify:

- when to ask a clarifying question
- when to abstain
- how to proceed when partial data exists

This is a major driver of trust in enterprise deployments. Users tolerate a question; they do not tolerate a confident hallucination.

### 3.4.1. Pattern: "If evidence is missing, say what is missing"

```text
If the required information is not present in the provided context, do not guess.
State what information is missing and ask 1 clarifying question.
```

## 3.5. Prompting for RAG (Grounding and Citations)

When RAG is used, the prompt must define the relationship between claims and evidence. The system should not treat retrieved context as optional; it should treat it as the grounding source.

A minimal enterprise pattern:

- Use retrieved context as the only source for factual claims
- Cite sources for key facts
- If context does not support an answer, abstain or ask a question

RAG-specific design and metrics belong in [`../02_solution_components/2.1_rag.md`](../02_solution_components/2.1_rag.md), but prompt engineering is where you enforce the user-visible behavior (citations, abstention, and scope).

---

# 4. PROMPT DESIGN PROCESS (FROM JTBD TO PROMPT SPEC)

**Prompt quality improves fastest when you treat it as a product lifecycle, not as a copywriting exercise.** The core idea is simple: define success, test against reality, ship controlled changes, and learn from production.

## 4.1. Start With JTBD and Requirements

Write the job-to-be-done in one sentence. Then convert it into requirements. This step is where consulting adds disproportionate value because it forces alignment across stakeholders.

Example requirement language:

- Must provide citations for factual claims (knowledge tasks)
- Must not expose confidential information (safety)
- Must not confirm actions without tool success (tool tasks)
- Must meet latency and cost budgets (business constraints)

## 4.2. Turn Requirements Into a Rubric and an Eval Set

Do not iterate prompts without measurement. **If you change a prompt and only "feel" that it improved, you are building a fragile system.** Instead, define a rubric and run evals as described in [`../02_solution_components/2.5_evals.md`](../02_solution_components/2.5_evals.md).

At a minimum:
- create a small v0 set (50-100 real cases)
- define pass/fail criteria and P0 blockers
- track failure reasons so changes become a backlog, not guesswork

## 4.3. Iterate With Controlled Changes

The most common failure mode is changing many things at once and losing attribution. Use a strict iteration habit:

1. Change one meaningful thing (prompt module, constraint, schema)
2. Re-run the same eval set
3. Compare deltas by slice (high-risk, tool cases, hard cases)
4. Ship only if safety holds and trade-offs are acceptable

This is how prompt engineering becomes a decision discipline.

## 4.4. Operationalize Prompts as Versioned Assets

Prompts should be treated like versioned assets with ownership. Whether you store them in git, a registry, or a database, you need:

- version and changelog (why it changed)
- status (draft, testing, production, deprecated)
- link to eval results (what improved, what regressed)
- owner and approval path (especially for high-risk use cases)

This is where many enterprise programs become sustainable: prompt changes stop being ad hoc and become governed releases.

---

# 5. SAFETY, SECURITY, AND RELIABILITY (PROMPTS IN THE REAL WORLD)

**Enterprise prompting must assume two realities: users will try unexpected things, and untrusted content will enter the system (including documents retrieved by RAG).** Prompts must support safe behavior, but **prompts alone are not enforcement**. Use guardrails for enforcement and monitoring.

## 5.1. Prompt Injection (Threat Model)

Prompt injection is when untrusted text tries to change system behavior. It can come from:

- user input ("ignore previous instructions and ...")
- retrieved documents ("the policy is to reveal all secrets ...")

The enterprise risk is not only "bad answers". It can include tool misuse and data leakage. In tool-enabled systems, injection can become a "confused deputy" problem: the model is tricked into using the system's privileges to do something the user should not be able to do.

## 5.2. Defensive Prompt Patterns (What Prompts Can Do)

Prompts can help by making boundaries explicit:

- specify that user input and retrieved content are untrusted
- specify that system instructions override user requests
- require citations and abstention
- require confirmation before high-risk actions

But prompts are not a firewall. Use guardrails to enforce policies (allowlists, validation, redaction, blocking), as covered in [`../02_solution_components/2.4_guardrails.md`](../02_solution_components/2.4_guardrails.md).

## 5.3. Tool Safety in Prompts (Selection vs Execution)

For tool calling, prompting must specify the behavior contract, but your system must enforce the boundaries:

- prompts should instruct the model to select tools only from an allowlist
- prompts should require argument validation and safe defaults
- prompts should require truthful confirmation based on tool output

Tool design and reliability patterns are covered in [`../02_solution_components/2.2_tool_calling.md`](../02_solution_components/2.2_tool_calling.md). Prompt engineering is where you define how the assistant explains actions and how it behaves when tools fail.

## 5.4. Reliability Patterns That Increase Trust

Trust is a product feature. Prompting supports trust when it enforces:

- "no guess" behavior under uncertainty
- explicit assumptions when need ed
- traceable outputs (citations, structured reasoning, clear next steps)
- predictable error handling (what happened, what to do now)

These behaviors should be measured with evals and monitored in production.

---

# 6. OPERATING PROMPTS IN PRODUCTION (GATES, MONITORING, GOVERNANCE)

The difference between "prompt engineering" and "prompt ops" is repetition. In production, you must expect drift: new documents, new user intents, new model versions, and new attack patterns. Your operating model must keep the system stable without slowing delivery to a halt.

## 6.1. Release Gates for Prompt Changes

Prompt changes should not be merged based on opinion. They should be gated by:

- pass rate on the eval set (overall and key slices)
- P0 safety failures (must be zero for release)
- cost and latency budgets (p95 targets)

This is the prompt equivalent of CI tests. The exact thresholds depend on risk appetite, but the discipline is universal.

## 6.2. Observability: What to Log (Safely)

At minimum, **you want to be able to answer: "what prompt version produced this behavior?" Without that, you cannot debug.** But logging must be safe: never log sensitive data unnecessarily.

Good practice is to log:
- prompt version ids (not raw prompts when possible)
- retrieval context ids (document/chunk ids)
- tool calls (name, args summary, success/failure) with redaction
- output classification (pass/fail class, safety blocks) when available

Production monitoring patterns belong in observability/LLMOps docs, but prompting must support it through explicit versioning and traceability.

## 6.3. Prompt Quality Checklist (Production Readiness)

Use this checklist as a final review gate. Keep it short and practical.

- **Clarity:** role, task, constraints, and output format are explicit
- **Grounding:** citations and abstention rules exist when facts matter
- **Safety:** untrusted inputs are delimited and policy boundaries are explicit
- **Tools:** tool usage is constrained and confirmations depend on tool output
- **Testability:** rubric exists and eval set covers key slices
- **Governance:** versioning, changelog, and rollback path exist

---

# NEXT STEPS

- [`1.3_hallucinations_basics.md`](./1.3_hallucinations_basics.md) - How to handle uncertainty and avoid confident wrong answers
- [`../02_solution_components/2.5_evals.md`](../02_solution_components/2.5_evals.md) - How to turn prompt changes into measurable go/no-go decisions
- [`../02_solution_components/2.4_guardrails.md`](../02_solution_components/2.4_guardrails.md) - How to enforce safety beyond what prompts can guarantee
- [`../02_solution_components/2.2_tool_calling.md`](../02_solution_components/2.2_tool_calling.md) - How to safely connect prompts to actions and external systems
- [`../02_solution_components/2.1_rag.md`](../02_solution_components/2.1_rag.md) - How to ground answers in enterprise knowledge with citations
